#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –≤ MoESystem.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –ó–∞–≥—Ä—É–∑–∫—É —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º
    - –†–∞–±–æ—Ç—É chat() —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏
    - –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src/python –≤ –ø—É—Ç—å
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src" / "python"))

from ui.backend.moe_system import MoESystem


def main():
    print("=" * 80)
    print("–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –≤ MoESystem")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MoESystem
    print("\n1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MoESystem...")
    moe = MoESystem(config_path="configs/ui_config.yaml")
    print("   ‚úÖ MoESystem –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞ general
    print("\n2. –ó–∞–≥—Ä—É–∑–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞ 'general' —Å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º...")
    success = moe.load_expert('general')

    if not success:
        print("   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞!")
        return 1

    print("   ‚úÖ –≠–∫—Å–ø–µ—Ä—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
    tokenizer = moe.tokenizers.get('general')
    if tokenizer is None:
        print("   ‚ùå –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1

    tokenizer_type = type(tokenizer).__name__
    print(f"   –¢–∏–ø —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞: {tokenizer_type}")
    print(f"   Vocab size: {len(tokenizer)}")

    # –¢–µ—Å—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –≤—Ö–æ–¥–æ–≤
    print("\n" + "=" * 80)
    print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ chat() —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤—Ö–æ–¥–∞–º–∏")
    print("=" * 80)

    test_cases = [
        {
            'name': '–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π',
            'message': '–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?'
        },
        {
            'name': '–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π',
            'message': 'Hello, how are you?'
        },
        {
            'name': '–†—É—Å—Å–∫–∏–π —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏',
            'message': '–ø—Ä–≤–∏–µ—Ç –∫–∫ –¥–µ–ª–∞ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ'
        },
        {
            'name': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏',
            'message': 'helo hw r u tody'
        },
        {
            'name': '–°–º–µ—à–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç',
            'message': '–ü—Ä–∏–≤–µ—Ç! Hello! How –¥–µ–ª–∞?'
        },
        {
            'name': '–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ',
            'message': '–Ω–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python'
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- –¢–µ—Å—Ç {i}: {test_case['name']} ---")
        print(f"User: {test_case['message']}")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        response = moe.chat(
            user_message=test_case['message'],
            expert_id='general'
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if 'error' in response or 'Error' in response.get('response', ''):
            print(f"‚ùå –û—à–∏–±–∫–∞: {response.get('response', 'Unknown error')}")
        else:
            print(f"Assistant: {response['response'][:100]}{'...' if len(response['response']) > 100 else ''}")
            print(f"–¢–æ–∫–µ–Ω–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {response['tokens_generated']}")
            print(f"Latency: {response['latency_ms']:.1f}ms")
            print(f"‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
    print("\n" + "=" * 80)
    print("4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏")
    print("=" * 80)

    memory_stats = moe.get_memory_stats()

    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
    print(f"  Total chunks: {memory_stats.get('total_chunks', 0)}")
    print(f"  Total tokens: {memory_stats.get('total_tokens', 0)}")

    if memory_stats.get('total_chunks', 0) > 0:
        print("  ‚úÖ –ü–∞–º—è—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        print("  ‚ö†Ô∏è  –ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞—è (–Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏
    print("\n" + "=" * 80)
    print("5. –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–ø–µ—á–∞—Ç–æ–∫")
    print("=" * 80)

    typo_tests = [
        ("–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–≤–∏–µ—Ç"),
        ("–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–∞–º–∏—Ä–æ–≤–∞–Ω–∏–µ"),
        ("–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–Ω–µ–π—Ä–æ–Ω–∞—è —Å–µ—Ç—å"),
        ("hello world", "helo wrld"),
    ]

    for correct, typo in typo_tests:
        tokens_correct = tokenizer.encode(correct, add_special_tokens=False)
        tokens_typo = tokenizer.encode(typo, add_special_tokens=False)

        unk_count_correct = sum(1 for t in tokens_correct if t == tokenizer.unk_token_id)
        unk_count_typo = sum(1 for t in tokens_typo if t == tokenizer.unk_token_id)

        print(f"\n–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ: '{correct}'")
        print(f"  –¢–æ–∫–µ–Ω–æ–≤: {len(tokens_correct)}, UNK: {unk_count_correct}")
        print(f"–° –æ–ø–µ—á–∞—Ç–∫–æ–π: '{typo}'")
        print(f"  –¢–æ–∫–µ–Ω–æ–≤: {len(tokens_typo)}, UNK: {unk_count_typo}")

        if unk_count_typo == 0:
            print("  ‚úÖ –û–ø–µ—á–∞—Ç–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –±–µ–∑ UNK")
        else:
            print(f"  ‚ö†Ô∏è  {unk_count_typo} UNK —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–ø–µ—á–∞—Ç–∫–µ")

    print("\n" + "=" * 80)
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    print("=" * 80)

    print("\n–í—ã–≤–æ–¥—ã:")
    print(f"  ‚Ä¢ –¢–∏–ø —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞: {tokenizer_type}")
    print(f"  ‚Ä¢ Vocabulary size: {len(tokenizer)}")
    print(f"  ‚Ä¢ BPE –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: {'‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç' if tokenizer_type == 'BPETokenizer' else '‚ö†Ô∏è SimpleTokenizer'}")
    print(f"  ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–µ—á–∞—Ç–æ–∫: {'‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è' if tokenizer_type == 'BPETokenizer' else '‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∞'}")
    print(f"  ‚Ä¢ –°–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏: {'‚úÖ –ê–∫—Ç–∏–≤–Ω–∞' if memory_stats.get('total_chunks', 0) > 0 else '‚ö†Ô∏è –ü—É—Å—Ç–∞—è'}")

    if tokenizer_type == 'BPETokenizer':
        print("\nüéâ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    else:
        print("\n‚ö†Ô∏è  BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –ù–ï –∑–∞–≥—Ä—É–∑–∏–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SimpleTokenizer")
        print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ tokenizer_config.json –≤ models/experts/general/")

    return 0


if __name__ == "__main__":
    sys.exit(main())
