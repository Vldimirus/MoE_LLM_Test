# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è ExpertModel - –û—Ç—á—ë—Ç

**–î–∞—Ç–∞:** 2026-01-07
**–í–µ—Ä—Å–∏—è:** 0.1.0
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω

---

## –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

### 1. PositionalEncoding ‚úÖ

**–§–∞–π–ª:** `src/python/models/expert.py`

–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤.

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (sin/cos)
- –ù–µ—Ç –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ buffer (—á–∞—Å—Ç—å state_dict)
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ max_seq_len

**–§–æ—Ä–º—É–ª—ã:**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**–¢–µ—Å—Ç:**
- –í—Ö–æ–¥: [2, 50, 512]
- –í—ã—Ö–æ–¥: [2, 50, 512]
- ‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã

### 2. ExpertModel ‚úÖ

**–§–∞–π–ª:** `src/python/models/expert.py`

–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è language model –Ω–∞ –±–∞–∑–µ Transformer.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
Input Token IDs
    ‚Üì
Token Embedding (vocab_size ‚Üí d_model)
    ‚Üì
Positional Encoding
    ‚Üì
TransformerBlock √ó N
    ‚Üì
Layer Normalization
    ‚Üì
LM Head (d_model ‚Üí vocab_size)
    ‚Üì
Output Logits
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `vocab_size`: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
- `d_model`: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings (default: 512)
- `n_layers`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ transformer –±–ª–æ–∫–æ–≤ (default: 6)
- `n_heads`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ attention heads (default: 8)
- `d_ff`: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN (default: 2048)
- `max_seq_len`: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (default: 2048)
- `dropout`: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout (default: 0.1)

**–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**

1. **`forward(input_ids, mask)`**
   - –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
   - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ causal mask –¥–ª—è autoregressive generation

2. **`generate(input_ids, max_new_tokens, temperature, top_k, top_p)`**
   - Autoregressive –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ temperature sampling
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ top-k sampling
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ nucleus (top-p) sampling
   - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

3. **`get_num_params(non_embedding)`**
   - –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –û–ø—Ü–∏—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è embedding –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

4. **`get_model_config()`**
   - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
   - –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è

5. **`_create_causal_mask(seq_len, device)`**
   - –°–æ–∑–¥–∞–Ω–∏–µ causal mask –¥–ª—è autoregressive generation
   - –ù–∏–∂–Ω—è—è —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞

---

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```python
vocab_size = 10,000
d_model = 512
n_layers = 6
n_heads = 8
d_ff = 2048
max_seq_len = 512
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**

‚úÖ **Forward Pass:**
- –í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [2, 20]
- –í—ã—Ö–æ–¥–Ω—ã–µ –ª–æ–≥–∏—Ç—ã: [2, 20, 10000]
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã: ‚úÖ

‚úÖ **Text Generation:**
- –ù–∞—á–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: 5
- –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: 10 –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
- Temperature sampling: ‚úÖ
- Top-k sampling: ‚úÖ
- Nucleus (top-p) sampling: ‚úÖ

‚úÖ **Positional Encoding:**
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è: ‚úÖ
- –ù–µ—Ç –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ‚úÖ

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:**
- –í—Å–µ–≥–æ: 29,155,328 (~29M)
- –ë–µ–∑ embedding: 24,035,328 (~24M)

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:**
- FP32: 111.2 MB
- FP16: 55.6 MB
- INT8: 27.8 MB

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚úÖ

```python
model = ExpertModel(vocab_size=5000, d_model=256, n_layers=4)

input_ids = torch.randint(0, 5000, (1, 10))
logits = model(input_ids)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å 5,719,552 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- –í—ã—Ö–æ–¥–Ω—ã–µ –ª–æ–≥–∏—Ç—ã: [1, 10, 5000]
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç

### –ü—Ä–∏–º–µ—Ä 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ‚úÖ

–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:

| –°—Ç—Ä–∞—Ç–µ–≥–∏—è | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã |
|-----------|-----------|
| Default | temperature=1.0 |
| Low temp | temperature=0.5 |
| High temp | temperature=1.5 |
| Top-k | temperature=0.8, top_k=20 |
| Top-p | temperature=0.8, top_p=0.9 |
| Combined | temperature=0.7, top_k=50, top_p=0.9 |

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### –ü—Ä–∏–º–µ—Ä 3: –†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π ‚úÖ

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | FP32 | FP16 | INT8 |
|--------|-----------|------|------|------|
| Tiny   | 1,676,800 | 6.4M | 3.2M | 1.6M |
| Small  | 8,279,552 | 31.6M | 15.8M | 7.9M |
| Medium | 51,683,328 | 197.2M | 98.6M | 49.3M |
| Large  | 133,504,512 | 509.3M | 254.6M | 127.3M |

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –†–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### –ü—Ä–∏–º–µ—Ä 4: –°–∫–æ—Ä–æ—Å—Ç—å inference ‚úÖ

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:** Medium model (29M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)

| Batch Size | –í—Ä–µ–º—è (ms) | Tok/s |
|------------|------------|-------|
| 1 | 156.85 | 318.8 |
| 2 | 119.84 | 834.4 |
| 4 | 286.75 | 697.5 |
| 8 | 629.98 | 634.9 |

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π batch size = 2 (834 tok/s)

### –ü—Ä–∏–º–µ—Ä 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–ó–∞–≥—Ä—É–∑–∫–∞ ‚úÖ

```python
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
checkpoint = {
    'model_state_dict': model.state_dict(),
    'config': model.get_model_config()
}
torch.save(checkpoint, 'model.pt')

# –ó–∞–≥—Ä—É–∑–∫–∞
checkpoint = torch.load('model.pt')
model = ExpertModel(**checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: ‚úÖ
- –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: ‚úÖ
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –≤—ã—Ö–æ–¥–∞—Ö: 0.0000000000
- ‚úÖ –ü–æ–ª–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –≤—ã—Ö–æ–¥–æ–≤

---

## –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

1. **`src/python/models/expert.py`** - –û—Å–Ω–æ–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
   - 450+ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
   - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º
   - Type hints –≤–µ–∑–¥–µ
   - –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞

2. **`docs/EXPERT_MODEL.md`** - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
   - –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
   - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞

3. **`examples/expert_model_example.py`** - –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - 5 –ø–æ–ª–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
   - –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã
   - –ì–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

4. **`src/python/models/__init__.py`** - –û–±–Ω–æ–≤–ª—ë–Ω
   - –ò–º–ø–æ—Ä—Ç—ã –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
   - –£–¥–æ–±–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª—è–º

---

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏

### ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

1. **Forward pass** - –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
2. **Autoregressive generation** - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
3. **Temperature sampling** - –∫–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
4. **Top-k sampling** - –≤—ã–±–æ—Ä –∏–∑ —Ç–æ–ø-K —Ç–æ–∫–µ–Ω–æ–≤
5. **Nucleus (top-p) sampling** - dynamic cutoff
6. **Causal masking** - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç attention –∫ –±—É–¥—É—â–µ–º—É
7. **Positional encoding** - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏—è—Ö
8. **State dict** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
9. **Parameter counting** - –ø–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
10. **Model configuration** - —ç–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### üöß –°–ª–µ–¥—É—é—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è

1. **KV-Cache** - –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
2. **Flash Attention** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è attention
3. **Gradient Checkpointing** - —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
4. **Mixed Precision** - FP16/BF16 training
5. **ONNX Export** - —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è production
6. **Quantization** - INT8/INT4 –¥–ª—è CPU inference
7. **Beam Search** - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
8. **Repetition Penalty** - –∏–∑–±–µ–≥–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

### 1. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings

```python
x = self.token_embedding(input_ids) * math.sqrt(self.d_model)
```

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:** –ò–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ Transformer paper - –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ.

### 2. Causal Mask

```python
mask = torch.tril(torch.ones(seq_len, seq_len))
```

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:** –ù–µ–æ–±—Ö–æ–¥–∏–º –¥–ª—è autoregressive generation - —Ç–æ–∫–µ–Ω –Ω–µ –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å –±—É–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã.

### 3. Positional Encoding –∫–∞–∫ Buffer

```python
self.register_buffer('pe', pe)
```

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**
- –ù–µ —è–≤–ª—è–µ—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º (–Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è)
- –ù–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ state_dict
- –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –Ω–∞ GPU –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—å—é

### 4. Shared Weights –º–µ–∂–¥—É Embedding –∏ LM Head

**–ù–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è –±—É–¥—É—â–µ–≥–æ):
```python
# –ú–æ–∂–Ω–æ —Å–≤—è–∑–∞—Ç—å –≤–µ—Å–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
self.lm_head.weight = self.token_embedding.weight
```

---

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –ù–∞—à–∞ Medium | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|--------|-----------|-------------|-------------|
| GPT-2 Small | 124M | 51M | –ú–µ–Ω—å—à–µ, –Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ |
| GPT-2 Medium | 355M | - | –ë–ª–∏–∑–∫–æ –∫ –Ω–∞—à–µ–π Large |
| BERT Base | 110M | 51M | –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ |
| DistilBERT | 66M | 51M | –ë–ª–∏–∑–∫–æ |

**–í—ã–≤–æ–¥:** –ù–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä–∞–º production –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è domain-specific —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.

---

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### CPU Inference (Ryzen 5 4500U)

**Medium model (29M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤):**
- Single token: ~160ms
- Batch processing: –¥–æ 834 tok/s
- –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π batch: 2-4

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –î–ª—è interactive inference: batch=1, ~300 tok/s
- –î–ª—è batch processing: batch=2-4, ~700-800 tok/s
- –î–ª—è production: –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è INT8 ‚Üí ~2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ

### –û—Ü–µ–Ω–∫–∞ GPU Performance

**–û–∂–∏–¥–∞–µ–º–æ–µ –Ω–∞ NVIDIA RTX 3060:**
- Medium model: ~5000-7000 tok/s
- Large model: ~2000-3000 tok/s
- –° mixed precision (FP16): +50% —Å–∫–æ—Ä–æ—Å—Ç—å

---

## –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞

### ‚úÖ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –ø—Ä–æ–µ–∫—Ç–∞

1. **Type hints** - –≤–µ–∑–¥–µ ‚úÖ
2. **Docstrings** - –Ω–∞ —Ä—É—Å—Å–∫–æ–º ‚úÖ
3. **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤** - –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö ‚úÖ
4. **Naming** - snake_case –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π, PascalCase –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ ‚úÖ
5. **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏** - –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ ‚úÖ
6. **–¢–µ—Å—Ç—ã** - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ‚úÖ
7. **–ü—Ä–∏–º–µ—Ä—ã** - –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ ‚úÖ
8. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –ø–æ–ª–Ω–∞—è MD —Ñ–∞–π–ª ‚úÖ

---

## –í—ã–≤–æ–¥—ã

### ‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

1. **ExpertModel** - –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è language model
2. **PositionalEncoding** - –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
3. **Text Generation** - –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ sampling
4. **Save/Load** - –ø–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ checkpoints
5. **Documentation** - –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
6. **Examples** - 5 —Ä–∞–±–æ—á–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤

### üéØ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

- ‚úÖ Inference: –≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
- ‚úÖ Generation: –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç
- ‚úÖ Save/Load: checkpoint —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç
- üöß Training: —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è training loop
- üöß Tokenizer: —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

- –¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã: ‚úÖ 100%
- –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç: ‚úÖ 100%
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: ‚úÖ –ü–æ–ª–Ω–∞—è
- Code coverage: ‚úÖ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø–æ–∫—Ä—ã—Ç

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)

1. **Training Loop** - —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
2. **Tokenizer Integration** - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
3. **Simple Router** - rule-based —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
4. **Dataset Loader** - –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ

5. **KV-Cache** - —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
6. **Expert Management** - —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏/–≤—ã–≥—Ä—É–∑–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
7. **Inference Engine** - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π inference pipeline
8. **Quantization** - INT8 –¥–ª—è CPU

### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ

9. **Flash Attention** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
10. **Distributed Training** - multi-GPU –æ–±—É—á–µ–Ω–∏–µ
11. **ONNX Export** - —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è production
12. **Benchmarks** - –¥–µ—Ç–∞–ª—å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ ExpertModel –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
**–î–∞—Ç–∞:** 2026-01-07
**–í–µ—Ä—Å–∏—è:** 0.1.0
