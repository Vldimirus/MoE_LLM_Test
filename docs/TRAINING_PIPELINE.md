## Training Pipeline Documentation

**–î–∞—Ç–∞:** 2026-01-07
**–í–µ—Ä—Å–∏—è:** 1.0.0
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ

---

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–û–±–∑–æ—Ä](#–æ–±–∑–æ—Ä)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
3. [–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã](#–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)
4. [–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](#–ø—Ä–∏–º–µ—Ä—ã-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
5. [API Reference](#api-reference)
6. [–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏](#–ª—É—á—à–∏–µ-–ø—Ä–∞–∫—Ç–∏–∫–∏)

---

## –û–±–∑–æ—Ä

Training Pipeline - –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (ExpertModel).

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

‚úÖ **Dataset Loading**
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ `.txt` –∏ `.jsonl` —Ñ–æ—Ä–º–∞—Ç–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- Sliding window –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
- Batching —Å padding

‚úÖ **Training Loop**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π backward pass
- Gradient accumulation
- Gradient clipping
- Learning rate scheduling (–≥–æ—Ç–æ–≤–æ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏)

‚úÖ **Validation & Metrics**
- Validation loop
- Loss –∏ Perplexity –º–µ—Ç—Ä–∏–∫–∏
- History tracking

‚úÖ **Checkpoint Management**
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ checkpoints
- Best model tracking
- Resume training

‚úÖ **Advanced Features**
- Early stopping
- –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è optimizer –∏ criterion
- Progress logging

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```
training/
‚îú‚îÄ‚îÄ dataset.py       # Data loading –∏ preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ SimpleTokenizer     # Word-level tokenizer
‚îÇ   ‚îú‚îÄ‚îÄ TextDataset         # Dataset –∫–ª–∞—Å—Å
‚îÇ   ‚îî‚îÄ‚îÄ create_dataloaders  # Helper –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DataLoaders
‚îÇ
‚îî‚îÄ‚îÄ trainer.py       # Training engine
    ‚îî‚îÄ‚îÄ Trainer      # –ì–ª–∞–≤–Ω—ã–π training loop
```

### Data Flow

```
–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (.txt/.jsonl)
    ‚Üì
SimpleTokenizer (build vocab)
    ‚Üì
TextDataset (tokenization + chunking)
    ‚Üì
DataLoader (batching + padding)
    ‚Üì
Trainer (training loop)
    ‚Üì
Trained Model + Checkpoints
```

---

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### 1. SimpleTokenizer

–ü—Ä–æ—Å—Ç–æ–π word-level —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è.

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –°–ª–æ–≤–∞—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ vocab

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```python
from training.dataset import SimpleTokenizer

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
tokenizer = SimpleTokenizer(vocab_size=10000)
texts = ["—ç—Ç–æ –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç", "—ç—Ç–æ –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç"]
tokenizer.build_vocab(texts)

# Encode/Decode
encoded = tokenizer.encode("—ç—Ç–æ —Ç–µ—Å—Ç")  # [2, 5, 7, 3]
decoded = tokenizer.decode(encoded)     # "—ç—Ç–æ —Ç–µ—Å—Ç"

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
print(f"Vocab size: {len(tokenizer)}")  # 10000
print(f"PAD token ID: {tokenizer.pad_token_id}")  # 0
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í production —ç—Ç–æ –±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ BPE tokenizer (GPT-2, SentencePiece).

---

### 2. TextDataset

PyTorch Dataset –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–§–æ—Ä–º–∞—Ç—ã –¥–∞–Ω–Ω—ã—Ö:**

**`.txt` —Ñ–∞–π–ª:**
```text
–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Ç–µ–∫—Å—Ç–∞.
–û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º.

–í—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –æ—Ç–¥–µ–ª—è–µ—Ç—Å—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.
```

**`.jsonl` —Ñ–∞–π–ª:**
```json
{"text": "–ü–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞"}
{"text": "–í—Ç–æ—Ä–æ–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞"}
{"text": "–¢—Ä–µ—Ç–∏–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞"}
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `file_path` | str | - | –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ |
| `tokenizer` | SimpleTokenizer | - | –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä |
| `max_length` | int | 512 | –ú–∞–∫—Å –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ |
| `stride` | int | 256 | –®–∞–≥ –¥–ª—è sliding window |

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```python
from training.dataset import TextDataset, SimpleTokenizer

tokenizer = SimpleTokenizer(vocab_size=10000)
tokenizer.build_vocab(texts)

dataset = TextDataset(
    file_path="data/train.txt",
    tokenizer=tokenizer,
    max_length=512
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ sample
sample = dataset[0]
# {'input_ids': Tensor[seq_len], 'labels': Tensor[seq_len]}
```

**Sliding Window:**

–î–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ `max_length`, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è sliding window:

```
–¢–µ–∫—Å—Ç: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
max_length=5, stride=3

Chunk 1: [0, 1, 2, 3, 4]
Chunk 2:       [3, 4, 5, 6, 7]
Chunk 3:             [6, 7, 8, 9]
```

–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã.

---

### 3. DataLoader Helper

–§—É–Ω–∫—Ü–∏—è `create_dataloaders()` —É–ø—Ä–æ—â–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ DataLoaders.

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```python
from training.dataset import create_dataloaders

train_loader, val_loader, tokenizer = create_dataloaders(
    train_file="data/train.txt",
    val_file="data/val.txt",  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    batch_size=8,
    max_length=512,
    num_workers=4
)

# –ì–æ—Ç–æ–≤—ã–µ DataLoaders –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä!
```

**Collate Function:**

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç padding:

```python
Batch (–¥–æ padding):
  Sample 1: [1, 2, 3, 4]
  Sample 2: [1, 2, 3, 4, 5, 6, 7]

Batch (–ø–æ—Å–ª–µ padding):
  input_ids: [[1, 2, 3, 4, 0, 0, 0],
              [1, 2, 3, 4, 5, 6, 7]]

  attention_mask: [[1, 1, 1, 1, 0, 0, 0],
                   [1, 1, 1, 1, 1, 1, 1]]

  labels: [[2, 3, 4, -100, -100, -100, -100],
           [2, 3, 4, 5, 6, 7, -100]]
```

Label `-100` –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≤ `CrossEntropyLoss`.

---

### 4. Trainer

–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `model` | nn.Module | - | –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |
| `train_dataloader` | DataLoader | - | Training data |
| `val_dataloader` | DataLoader | None | Validation data |
| `optimizer` | Optimizer | AdamW | –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä |
| `criterion` | Loss | CrossEntropyLoss | Loss function |
| `device` | str | 'cpu' | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cpu'/'cuda') |
| `gradient_accumulation_steps` | int | 1 | Gradient accumulation |
| `max_grad_norm` | float | 1.0 | Gradient clipping |
| `checkpoint_dir` | str | 'checkpoints' | –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è checkpoints |
| `log_interval` | int | 10 | –ò–Ω—Ç–µ—Ä–≤–∞–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è |

**–ú–µ—Ç–æ–¥—ã:**

```python
# –û–±—É—á–µ–Ω–∏–µ
trainer.train(
    num_epochs=10,
    save_every=1,
    early_stopping_patience=3
)

# –í–∞–ª–∏–¥–∞—Ü–∏—è
val_metrics = trainer.validate()

# Checkpoint management
trainer.save_checkpoint(path)
trainer.load_checkpoint(path)

# –ò—Å—Ç–æ—Ä–∏—è
trainer.save_history(path)
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```python
from models.expert import ExpertModel
from training.dataset import create_dataloaders
from training.trainer import Trainer

# 1. –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders
train_loader, _, tokenizer = create_dataloaders(
    train_file="data/train.txt",
    batch_size=8,
    max_length=512
)

# 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = ExpertModel(
    vocab_size=len(tokenizer),
    d_model=512,
    n_layers=8,
    n_heads=8,
    d_ff=2048,
    max_seq_len=512
)

# 3. –°–æ–∑–¥–∞–Ω–∏–µ Trainer
trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    device='cpu'
)

# 4. –û–±—É—á–µ–Ω–∏–µ
history = trainer.train(num_epochs=10)

print(f"Final loss: {history['train_loss'][-1]:.4f}")
```

---

### –ü—Ä–∏–º–µ—Ä 2: –û–±—É—á–µ–Ω–∏–µ —Å Validation

```python
# –î–æ–±–∞–≤–ª—è–µ–º validation set
train_loader, val_loader, tokenizer = create_dataloaders(
    train_file="data/train.txt",
    val_file="data/val.txt",  # Validation data
    batch_size=8,
    max_length=512
)

model = ExpertModel(...)

trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    val_dataloader=val_loader,  # –î–æ–±–∞–≤–ª—è–µ–º validation
    device='cpu'
)

history = trainer.train(num_epochs=10)

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
for epoch, (train_loss, val_loss) in enumerate(
    zip(history['train_loss'], history['val_loss']), 1
):
    print(f"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}")
```

---

### –ü—Ä–∏–º–µ—Ä 3: Early Stopping

```python
trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    device='cpu'
)

# Early stopping –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è 3 —ç–ø–æ—Ö–∏
history = trainer.train(
    num_epochs=50,
    early_stopping_patience=3
)

print(f"Stopped at epoch: {trainer.current_epoch}")
print(f"Best val loss: {trainer.best_val_loss:.4f}")
```

---

### –ü—Ä–∏–º–µ—Ä 4: –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è

```python
import torch.optim as optim
import torch.nn as nn

# –ö–∞—Å—Ç–æ–º–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.98),
    weight_decay=0.01
)

# –ö–∞—Å—Ç–æ–º–Ω—ã–π loss
criterion = nn.CrossEntropyLoss(
    ignore_index=-100,
    label_smoothing=0.1  # Label smoothing
)

trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    optimizer=optimizer,
    criterion=criterion,
    gradient_accumulation_steps=4,  # Gradient accumulation
    max_grad_norm=0.5,  # Gradient clipping
    device='cuda'  # GPU
)

history = trainer.train(num_epochs=20)
```

---

### –ü—Ä–∏–º–µ—Ä 5: Resume Training

```python
# –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
trainer1 = Trainer(model, train_loader, checkpoint_dir='checkpoints')
trainer1.train(num_epochs=10)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint
checkpoint_path = Path('checkpoints/my_checkpoint.pt')
trainer1.save_checkpoint(checkpoint_path)

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
new_model = ExpertModel(...)
trainer2 = Trainer(new_model, train_loader)
trainer2.load_checkpoint(checkpoint_path)

# –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —ç–ø–æ—Ö–∏ 11
trainer2.train(num_epochs=10)
```

---

## API Reference

### SimpleTokenizer

```python
class SimpleTokenizer:
    def __init__(self, vocab_size: int = 10000)
    def build_vocab(self, texts: List[str]) -> None
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str
    def __len__(self) -> int
```

**–ê—Ç—Ä–∏–±—É—Ç—ã:**
- `pad_token_id: int` - ID PAD —Ç–æ–∫–µ–Ω–∞ (0)
- `unk_token_id: int` - ID UNK —Ç–æ–∫–µ–Ω–∞ (1)
- `bos_token_id: int` - ID BOS —Ç–æ–∫–µ–Ω–∞ (2)
- `eos_token_id: int` - ID EOS —Ç–æ–∫–µ–Ω–∞ (3)
- `word2idx: Dict[str, int]` - –°–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–æ ‚Üí ID
- `idx2word: Dict[int, str]` - –°–ª–æ–≤–∞—Ä—å ID ‚Üí —Å–ª–æ–≤–æ

---

### TextDataset

```python
class TextDataset(Dataset):
    def __init__(
        self,
        file_path: str,
        tokenizer: SimpleTokenizer,
        max_length: int = 512,
        stride: int = 256
    )
    def __len__(self) -> int
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]
```

**Returns (`__getitem__`):**
```python
{
    'input_ids': Tensor[seq_len],  # –í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    'labels': Tensor[seq_len]      # –¶–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (—Å–º–µ—â–µ–Ω—ã –Ω–∞ 1)
}
```

---

### Trainer

```python
class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_dataloader: DataLoader,
        val_dataloader: Optional[DataLoader] = None,
        optimizer: Optional[optim.Optimizer] = None,
        criterion: Optional[nn.Module] = None,
        device: str = "cpu",
        gradient_accumulation_steps: int = 1,
        max_grad_norm: float = 1.0,
        checkpoint_dir: str = "checkpoints",
        log_interval: int = 10
    )

    def train(
        self,
        num_epochs: int,
        save_every: int = 1,
        early_stopping_patience: Optional[int] = None
    ) -> Dict[str, List[float]]

    def validate(self) -> Dict[str, float]
    def save_checkpoint(self, path: Path, is_best: bool = False) -> None
    def load_checkpoint(self, path: Path) -> None
    def save_history(self, path: Path) -> None
```

**–ê—Ç—Ä–∏–±—É—Ç—ã:**
- `current_epoch: int` - –¢–µ–∫—É—â–∞—è —ç–ø–æ—Ö–∞
- `global_step: int` - –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
- `best_val_loss: float` - –õ—É—á—à–∏–π validation loss
- `history: Dict[str, List[float]]` - –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è

---

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### 1. –í—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**Batch size:**
- CPU: 2-8 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç RAM)
- GPU: 16-64 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç VRAM)
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient accumulation –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö batches

**Max sequence length:**
- Short texts: 128-256
- Medium texts: 512
- Long texts: 1024-2048

**Learning rate:**
- –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å `5e-4` (AdamW)
- –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π: `1e-4 - 3e-4`
- –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö: `5e-4 - 1e-3`

**Gradient accumulation:**
```python
effective_batch_size = batch_size * gradient_accumulation_steps

# –ü—Ä–∏–º–µ—Ä: batch_size=4, accumulation=4 ‚Üí effective batch=16
```

---

### 2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**
- ‚úÖ Train loss —Å—Ç–∞–±–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è
- ‚úÖ Perplexity —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è
- ‚úÖ Val loss –±–ª–∏–∑–æ–∫ –∫ train loss (–Ω–µ—Ç overfitting)

**–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–æ–±–ª–µ–º:**
- ‚ö†Ô∏è Loss –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è ‚Üí —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π LR
- ‚ö†Ô∏è Loss = NaN ‚Üí —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π LR, –≤–∑–æ—Ä–≤–∞–ª–∏—Å—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
- ‚ö†Ô∏è Val loss >> Train loss ‚Üí overfitting
- ‚ö†Ô∏è Train loss >> Val loss ‚Üí –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏

---

### 3. Checkpoint Strategy

```python
# –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ:
# 1. –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ checkpoints (–∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö)
trainer.train(num_epochs=100, save_every=5)

# 2. Best model (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ early stopping)
trainer.train(num_epochs=100, early_stopping_patience=5)

# 3. Final model (–≤—Ä—É—á–Ω—É—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)
trainer.save_checkpoint(Path('checkpoints/final_model.pt'))
```

---

### 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**CPU:**
```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ num_workers –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
train_loader = DataLoader(..., num_workers=4)

# Gradient accumulation –¥–ª—è –±–æ–ª—å—à–∏—Ö effective batch
trainer = Trainer(..., gradient_accumulation_steps=4)
```

**GPU (–∫–æ–≥–¥–∞ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ):**
```python
# Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# –í training loop:
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

### 5. –û—Ç–ª–∞–¥–∫–∞

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:**
```python
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π batch
batch = next(iter(train_loader))
print(f"Input shape: {batch['input_ids'].shape}")
print(f"Labels shape: {batch['labels'].shape}")
print(f"Input IDs: {batch['input_ids'][0][:10]}")  # –ü–µ—Ä–≤—ã–µ 10 —Ç–æ–∫–µ–Ω–æ–≤
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏:**
```python
# –û–¥–∏–Ω forward pass
model.eval()
with torch.no_grad():
    logits = model(batch['input_ids'])
    print(f"Logits shape: {logits.shape}")  # [batch, seq_len, vocab_size]
```

**Overfitting test:**
```python
# –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è –Ω–∞ 1 batch
small_loader = DataLoader(dataset, batch_size=1)
trainer = Trainer(model, small_loader)
history = trainer.train(num_epochs=100)

# Loss –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ ~0
assert history['train_loss'][-1] < 0.1, "–ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è!"
```

---

## –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

1. **SimpleTokenizer** - word-level —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
   - ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è production
   - ‚úÖ –•–æ—Ä–æ—à –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è
   - üîÑ –ë—É–¥–µ—Ç –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ BPE tokenizer

2. **Validation metrics** - —Ç–æ–ª—å–∫–æ loss –∏ perplexity
   - üîÑ –í –±—É–¥—É—â–µ–º: BLEU, ROUGE, accuracy

3. **Learning rate scheduling** - –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
   - üîÑ –í –ø–ª–∞–Ω–∞—Ö: cosine annealing, warmup

---

## Roadmap

### –ë–ª–∏–∂–∞–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è

- [ ] BPE Tokenizer –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (GPT-2, SentencePiece)
- [ ] Learning rate scheduler (warmup + cosine annealing)
- [ ] –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (BLEU, ROUGE)
- [ ] Distributed training (multi-GPU)
- [ ] Mixed precision training (FP16)
- [ ] Tensorboard integration
- [ ] Gradient checkpointing –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

Training Pipeline –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ExpertModel!

‚úÖ **–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ:**
- Dataset loading (.txt, .jsonl)
- Training loop —Å validation
- Checkpoint management
- Early stopping
- 6 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

üìä **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**
- ‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã
- ‚úÖ Loss —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (3.76 ‚Üí 2.68)
- ‚úÖ Checkpoint save/load —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ Early stopping —Ä–∞–±–æ—Ç–∞–µ—Ç

üöÄ **–ì–æ—Ç–æ–≤–æ –∫ production:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ datasets
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç resume training

---

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2026-01-07
**–í–µ—Ä—Å–∏—è:** 1.0.0
**–ê–≤—Ç–æ—Ä:** Vladimir (—Å –ø–æ–º–æ—â—å—é Claude Code)
