# Конфигурация Transfer Learning из GGUF моделей
#
# Этот файл определяет параметры для переноса знаний из GGUF модели
# (Phi-3, Llama, Mistral, и т.д.) в ExpertModel проекта NM_LLM_Test_2.
#
# Использование:
#   python scripts/import_gguf_expert.py \
#     --config configs/transfer_learning_config.yaml \
#     --expert_id my_expert

# ============================================================================
# GGUF модель (источник знаний)
# ============================================================================
gguf_model:
  # Путь к GGUF файлу
  # Примеры:
  #   - models/gguf/phi-3-mini-4k-instruct-q8_0.gguf
  #   - models/gguf/mistral-7b-instruct-v0.2-q8_0.gguf
  #   - models/gguf/llama-2-7b-chat-q8_0.gguf
  path: "models/gguf/phi-3-mini-4k-instruct-q8_0.gguf"

# ============================================================================
# Целевая модель (ExpertModel)
# ============================================================================
target_model:
  # Размер vocabulary (BPE токенайзер)
  # Обычно: 8000 для мультиязычного BPE
  # ВАЖНО: Для Llama-3.2 рекомендуется увеличить до 16000-32000
  vocab_size: 8000

  # Размерность embeddings
  # ⚠️ КРИТИЧНО: Для Llama-3.2 (d_model=2048) используйте минимум 1024!
  # Рекомендуемые значения:
  #   - 512: только для маленьких моделей (< 1B параметров)
  #   - 1024: для Llama-1B с умеренной потерей качества
  #   - 2048: для полного переноса знаний Llama-3.2 (рекомендуется!)
  #   - 4096: для максимального качества (требует GPU)
  d_model: 2048  # ← Увеличено для совместимости с Llama-3.2!

  # Количество transformer слоёв
  # Llama-3.2-1B имеет 16 слоёв, можем взять меньше
  # Рекомендуемые значения:
  #   - 6-8: для баланса качество/скорость
  #   - 12-16: для лучшего качества
  n_layers: 8  # ← Увеличено с 6 до 8

  # Количество attention heads
  # Llama-3.2 имеет 32 heads, но можем использовать меньше
  # Должно делить d_model нацело: 2048 % 16 = 128 (ok!)
  n_heads: 16  # ← Увеличено с 8 до 16 для d_model=2048

  # Размерность feed-forward слоя
  # Обычно: 4 * d_model
  d_ff: 8192  # ← 4 * 2048

  # Максимальная длина последовательности
  # Рекомендуемые значения:
  #   - 512: для коротких диалогов
  #   - 1024-2048: для стандартных задач
  #   - 4096: для длинных контекстов
  max_seq_len: 512

  # Dropout для regularization
  # Обычно: 0.1 для обучения, 0.0 для inference
  dropout: 0.1

# ============================================================================
# Параметры transfer learning
# ============================================================================
transfer:
  # Список индексов слоёв для переноса
  # Примеры:
  #   - [0, 1, 2, 3]: только первые 4 слоя
  #   - [0, 1, 2, 3, 4, 5]: все 6 слоёв (если target_model.n_layers=6)
  #   - null: автоматически переносятся все доступные слои
  layers_to_transfer: [0, 1, 2, 3, 4, 5]

  # Заморозить перенесённые веса
  # true: перенесённые параметры не обучаются (рекомендуется для начала)
  # false: все параметры обучаются (может привести к catastrophic forgetting)
  freeze_transferred: true

  # Адаптировать embeddings под BPE vocabulary
  # true: создаёт mapping GGUF vocab → BPE vocab (рекомендуется)
  # false: копирует embeddings как есть (может не работать при vocab mismatch)
  align_embeddings: true

  # Стратегия при несовпадении vocabulary size
  # resize: создаёт новую матрицу embeddings, копирует совпадающие токены
  # error: выдаёт ошибку при несовпадении
  # skip: пропускает embeddings (использует random initialization)
  on_vocab_mismatch: "resize"

  # Стратегия при несовпадении d_model
  # interpolate: интерполирует размерность (рекомендуется)
  # truncate: обрезает лишние размерности
  # error: выдаёт ошибку при несовпадении
  on_dimension_mismatch: "interpolate"

# ============================================================================
# Токенайзер
# ============================================================================
tokenizer:
  # Путь к BPE токенайзеру
  path: "models/tokenizers/bpe_multilang.model"

  # Параметры токенайзера
  vocab_size: 8000
  pad_token_id: 0
  unk_token_id: 1
  bos_token_id: 2
  eos_token_id: 3

# ============================================================================
# Управление памятью
# ============================================================================
memory:
  # Максимальное использование RAM (в GB)
  # Рекомендуется: оставить 2-4 GB для системы
  # Примеры:
  #   - 12: для систем с 16GB RAM
  #   - 8: для систем с 12GB RAM
  #   - 4: для систем с 8GB RAM
  max_ram_gb: 12

  # Резерв RAM, который всегда оставляем свободным
  safety_margin_gb: 2.0

  # Использовать memory-mapped files для GGUF
  # true: не загружаем весь GGUF файл в RAM (рекомендуется)
  # false: загружаем весь файл (быстрее, но требует больше RAM)
  use_mmap: true

  # Offload тензоров на диск при нехватке памяти
  # false: для MVP (Фаза 1)
  # true: будет реализовано в Фазе 3
  offload_to_disk: false

# ============================================================================
# Настройки вывода (логирование)
# ============================================================================
logging:
  # Уровень детализации логов
  # INFO: стандартный вывод (рекомендуется)
  # DEBUG: детальный вывод для отладки
  # WARNING: только предупреждения и ошибки
  level: "INFO"

  # Выводить размерности тензоров
  show_tensor_shapes: true

  # Выводить отчёт о памяти
  show_memory_report: true

# ============================================================================
# Примеры конфигураций для разных сценариев
# ============================================================================

# Пример 1: Быстрый эксперимент (маленькая модель)
# target_model:
#   vocab_size: 8000
#   d_model: 256
#   n_layers: 4
#   n_heads: 8
#   d_ff: 1024
#   max_seq_len: 512

# Пример 2: Хорошее качество на CPU
# target_model:
#   vocab_size: 8000
#   d_model: 1024
#   n_layers: 8
#   n_heads: 16
#   d_ff: 4096
#   max_seq_len: 1024

# Пример 3: Максимальное качество (требует GPU)
# target_model:
#   vocab_size: 8000
#   d_model: 2048
#   n_layers: 12
#   n_heads: 32
#   d_ff: 8192
#   max_seq_len: 2048
