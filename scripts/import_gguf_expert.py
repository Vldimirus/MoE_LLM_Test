#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è expert –º–æ–¥–µ–ª–∏ –∏–∑ GGUF —Ñ–∞–π–ª–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Transfer Learning Pipeline –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ GGUF –º–æ–¥–µ–ª–∏
(Phi-3, Llama, Mistral, –∏ —Ç.–¥.) –≤ ExpertModel –ø—Ä–æ–µ–∫—Ç–∞ NM_LLM_Test_2.

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞:
    python scripts/import_gguf_expert.py \
        --gguf models/gguf/phi-3-mini-q8.gguf \
        --config configs/transfer_learning_config.yaml \
        --output models/experts/phi3_expert \
        --expert_id phi3_expert

2. –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
    python scripts/import_gguf_expert.py \
        --gguf models/gguf/phi-3-mini-q8.gguf \
        --expert_id my_expert \
        --output models/experts/my_expert \
        --d_model 512 \
        --n_layers 6 \
        --n_heads 8

3. –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏):
    python scripts/import_gguf_expert.py \
        --gguf models/gguf/phi-3-mini-q8.gguf \
        --config configs/transfer_learning_config.yaml \
        --check_only

4. –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å –Ω–µ–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏:
    python scripts/import_gguf_expert.py \
        --gguf models/gguf/mistral-7b-q8.gguf \
        --expert_id mistral_expert \
        --output models/experts/mistral_expert \
        --no_freeze
"""

import argparse
import sys
import json
import torch
from pathlib import Path
from typing import Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src/python
sys.path.insert(0, str(Path(__file__).parent.parent / "src" / "python"))

from transfer_learning import TransferLearningPipeline
from utils.config_loader import load_config


def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="–ò–º–ø–æ—Ä—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏–∑ GGUF –º–æ–¥–µ–ª–∏",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    parser.add_argument(
        "--gguf",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ GGUF —Ñ–∞–π–ª—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: models/gguf/phi-3-mini-q8.gguf)"
    )

    parser.add_argument(
        "--expert_id",
        type=str,
        required=True,
        help="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: python_expert)"
    )

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    parser.add_argument(
        "--config",
        type=str,
        default="configs/transfer_learning_config.yaml",
        help="–ü—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (default: configs/transfer_learning_config.yaml)"
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∞ (default: models/experts/{expert_id})"
    )

    parser.add_argument(
        "--name",
        type=str,
        default=None,
        help="–ß–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è —ç–∫—Å–ø–µ—Ä—Ç–∞ (default: expert_id)"
    )

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
    parser.add_argument(
        "--d_model",
        type=int,
        default=None,
        help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)"
    )

    parser.add_argument(
        "--n_layers",
        type=int,
        default=None,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ transformer —Å–ª–æ—ë–≤ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)"
    )

    parser.add_argument(
        "--n_heads",
        type=int,
        default=None,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ attention heads (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)"
    )

    parser.add_argument(
        "--d_ff",
        type=int,
        default=None,
        help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å feed-forward —Å–ª–æ—è (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)"
    )

    parser.add_argument(
        "--vocab_size",
        type=int,
        default=None,
        help="–†–∞–∑–º–µ—Ä vocabulary (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)"
    )

    # Transfer learning –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument(
        "--layers",
        type=str,
        default=None,
        help="–ò–Ω–¥–µ–∫—Å—ã —Å–ª–æ—ë–≤ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '0,1,2,3')"
    )

    parser.add_argument(
        "--no_freeze",
        action="store_true",
        help="–ù–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–µ –≤–µ—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è)"
    )

    parser.add_argument(
        "--no_align_embeddings",
        action="store_true",
        help="–ù–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å embeddings –ø–æ–¥ BPE vocab"
    )

    # –£—Ç–∏–ª–∏—Ç—ã
    parser.add_argument(
        "--check_only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ (DEBUG logging)"
    )

    parser.add_argument(
        "--max_ram_gb",
        type=float,
        default=12.0,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM –≤ GB (default: 12.0)"
    )

    return parser.parse_args()


def load_or_create_config(args) -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ CLI –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤.

    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ñ–∏–≥
    try:
        config = load_config(args.config)
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: {args.config}\n")
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.config}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n")
        config = {
            'target_model': {
                'vocab_size': 8000,
                'd_model': 2048,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Llama-3.2
                'n_layers': 8,
                'n_heads': 16,
                'd_ff': 8192,
                'max_seq_len': 512,
                'dropout': 0.1
            },
            'transfer': {
                'layers_to_transfer': None,
                'freeze_transferred': True,
                'align_embeddings': True
            },
            'tokenizer': {
                'path': 'models/tokenizers/bpe_multilang.model'
            }
        }

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ CLI
    if args.d_model is not None:
        config['target_model']['d_model'] = args.d_model
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: d_model = {args.d_model}")

    if args.n_layers is not None:
        config['target_model']['n_layers'] = args.n_layers
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: n_layers = {args.n_layers}")

    if args.n_heads is not None:
        config['target_model']['n_heads'] = args.n_heads
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: n_heads = {args.n_heads}")

    if args.d_ff is not None:
        config['target_model']['d_ff'] = args.d_ff
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: d_ff = {args.d_ff}")

    if args.vocab_size is not None:
        config['target_model']['vocab_size'] = args.vocab_size
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: vocab_size = {args.vocab_size}")

    if args.layers is not None:
        layers = [int(x.strip()) for x in args.layers.split(',')]
        config['transfer']['layers_to_transfer'] = layers
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: layers_to_transfer = {layers}")

    if args.no_freeze:
        config['transfer']['freeze_transferred'] = False
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: freeze_transferred = False")

    if args.no_align_embeddings:
        config['transfer']['align_embeddings'] = False
        print(f"   –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: align_embeddings = False")

    return config


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    args = parse_args()

    print("\n" + "="*70)
    print("üöÄ Import GGUF Expert - Transfer Learning Pipeline")
    print("="*70 + "\n")

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è GGUF —Ñ–∞–π–ª–∞
    gguf_path = Path(args.gguf)
    if not gguf_path.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: GGUF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.gguf}")
        print(f"\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        sys.exit(1)

    print(f"üì¶ GGUF –º–æ–¥–µ–ª—å: {gguf_path.name}")
    print(f"   –†–∞–∑–º–µ—Ä: {gguf_path.stat().st_size / (1024**3):.2f} GB")
    print(f"   –ü—É—Ç—å: {gguf_path}\n")

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_or_create_config(args)

    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if args.output is None:
        output_dir = Path(f"models/experts/{args.expert_id}")
    else:
        output_dir = Path(args.output)

    expert_name = args.name if args.name else args.expert_id

    print(f"üéØ –¶–µ–ª–µ–≤–æ–π —ç–∫—Å–ø–µ—Ä—Ç:")
    print(f"   ID: {args.expert_id}")
    print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {expert_name}")
    print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}\n")

    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transfer Learning Pipeline
    try:
        pipeline = TransferLearningPipeline(
            gguf_path=str(gguf_path),
            target_model_config=config['target_model'],
            bpe_tokenizer_path=config.get('tokenizer', {}).get('path'),
            max_ram_gb=args.max_ram_gb
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ pipeline:")
        print(f"   {str(e)}\n")
        sys.exit(1)

    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    print("\n" + "="*70)
    print("üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
    print("="*70 + "\n")

    compat = pipeline.validate_compatibility()

    if not compat['compatible']:
        print("\n‚ùå –ú–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞!")
        print("\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for i, warning in enumerate(compat['warnings'], 1):
            print(f"  {i}. {warning}")

        print("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å anyway? (y/n): ", end='')
        response = input().lower()

        if response != 'y':
            print("\n‚ö†Ô∏è –ò–º–ø–æ—Ä—Ç –æ—Ç–º–µ–Ω—ë–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            sys.exit(0)

    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∑–∞–≤–µ—Ä—à–∞–µ–º
    if args.check_only:
        print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (--check_only)")
        print(f"   –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: {'‚úÖ –î–∞' if compat['compatible'] else '‚ùå –ù–µ—Ç'}")
        print(f"   Vocab overlap: {compat['vocab_overlap']:.1%}")
        print(f"   Transferable layers: {compat['transferable_layers']}")
        sys.exit(0)

    # 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ GGUF
    print("\n" + "="*70)
    print("üîÑ –ü–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –∏–∑ GGUF –≤ ExpertModel")
    print("="*70 + "\n")

    try:
        model = pipeline.initialize_model_from_gguf(
            layers_to_transfer=config['transfer'].get('layers_to_transfer'),
            freeze_transferred_layers=config['transfer'].get('freeze_transferred', True),
            align_embeddings=config['transfer'].get('align_embeddings', True)
        )
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å–µ –≤–µ—Å–æ–≤:")
        print(f"   {str(e)}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n" + "="*70)
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞")
    print("="*70 + "\n")

    output_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º model checkpoint
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'config': config['target_model'],
        'expert_id': args.expert_id,
        'expert_name': expert_name,
        'source_gguf': str(gguf_path),
        'transfer_config': config['transfer']
    }

    checkpoint_path = output_dir / "model.pt"
    torch.save(checkpoint, checkpoint_path)
    print(f"‚úÖ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º metadata
    metadata = {
        "expert_id": args.expert_id,
        "name": expert_name,
        "type": "transferred_model",
        "version": "1.0.0-gguf",
        "source_gguf": str(gguf_path),
        "gguf_metadata": compat['gguf_metadata'],
        "vocab_overlap": compat['vocab_overlap'],
        "transferred_layers": config['transfer'].get('layers_to_transfer'),
        "freeze_transferred": config['transfer'].get('freeze_transferred', True),
        "architecture": config['target_model'],
        "trainable_params_M": sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6,
        "total_params_M": sum(p.numel() for p in model.parameters()) / 1e6
    }

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Metadata —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {metadata_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º tokenizer config
    tokenizer_config = {
        "tokenizer_type": "bpe",
        "model_path": config.get('tokenizer', {}).get('path', 'models/tokenizers/bpe_multilang.model'),
        "vocab_size": config['target_model']['vocab_size'],
        "pad_token_id": 0,
        "unk_token_id": 1,
        "bos_token_id": 2,
        "eos_token_id": 3
    }

    tokenizer_config_path = output_dir / "tokenizer_config.json"
    with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Tokenizer config —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {tokenizer_config_path}")

    # 8. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
    print("\n" + "="*70)
    print("üéâ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
    print("="*70 + "\n")

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   Expert ID: {args.expert_id}")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {metadata['total_params_M']:.2f}M")
    print(f"   Trainable: {metadata['trainable_params_M']:.2f}M ({100*metadata['trainable_params_M']/metadata['total_params_M']:.1f}%)")
    print(f"   Vocab overlap: {metadata['vocab_overlap']:.1%}")
    print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")

    print(f"\nüìù –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n")
    print(f"1. Fine-tuning —ç–∫—Å–ø–µ—Ä—Ç–∞ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   python scripts/train_simple.py --expert {args.expert_id} --data your_data.jsonl\n")
    print(f"2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ UI:")
    print(f"   python src/python/ui/app.py")
    print(f"   –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É 'üí¨ Chat' –∏ –≤—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ '{args.expert_id}'\n")
    print(f"3. Inference —á–µ—Ä–µ–∑ API:")
    print(f"   from python.models.expert import ExpertModel")
    print(f"   model = ExpertModel.from_pretrained('{output_dir}')\n")

    print("="*70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C)")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:")
        print(f"   {str(e)}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)
