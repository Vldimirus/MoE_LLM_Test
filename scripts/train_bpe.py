#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ general —Å –Ω–æ–≤—ã–º BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä: models/tokenizers/bpe_multilang.model
    - –î–∞—Ç–∞—Å–µ—Ç: data/training/corpus_for_bpe.txt
    - Vocab size: 8000 (BPE)
    - –ú–æ–¥–µ–ª—å: ExpertModel —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src/python –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src" / "python"))

import torch
from torch.utils.data import Dataset, DataLoader
from models.expert import ExpertModel
from training.bpe_tokenizer import BPETokenizer
from training.trainer import Trainer
from typing import List, Dict


class BPETextDataset(Dataset):
    """
    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º.

    –ê–Ω–∞–ª–æ–≥ TextDataset, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç BPETokenizer –≤–º–µ—Å—Ç–æ SimpleTokenizer.
    """

    def __init__(
        self,
        file_path: str,
        tokenizer: BPETokenizer,
        max_length: int = 128,
        stride: int = 64
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è dataset.

        Args:
            file_path: –ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É (.txt)
            tokenizer: BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            stride: –®–∞–≥ –¥–ª—è sliding window
        """
        self.file_path = Path(file_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.stride = stride
        self.samples: List[List[int]] = []

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._load_data()

    def _load_data(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ preprocessing –¥–∞–Ω–Ω—ã—Ö."""
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {self.file_path}...")

        # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        with open(self.file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]

        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(lines)} —Å—Ç—Ä–æ–∫ —Ç–µ–∫—Å—Ç–∞")

        # Tokenization –∏ —Å–æ–∑–¥–∞–Ω–∏–µ samples
        for text in lines:
            # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å BPE
            tokens = self.tokenizer.encode(
                text,
                add_special_tokens=True,
                max_length=None,
                padding=False,
                truncation=False
            )

            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ max_length, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks —Å overlap
            if len(tokens) > self.max_length:
                for i in range(0, len(tokens) - self.max_length + 1, self.stride):
                    chunk = tokens[i:i + self.max_length]
                    self.samples.append(chunk)
            else:
                self.samples.append(tokens)

        print(f"–°–æ–∑–¥–∞–Ω–æ {len(self.samples)} samples –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    def __len__(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ samples –≤ dataset."""
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ sample.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å input_ids –∏ labels
        """
        tokens = self.samples[idx]

        # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è: input = tokens[:-1], target = tokens[1:]
        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        labels = torch.tensor(tokens[1:], dtype=torch.long)

        return {
            'input_ids': input_ids,
            'labels': labels
        }


def collate_fn(batch: List[Dict[str, torch.Tensor]], pad_token_id: int = 0) -> Dict[str, torch.Tensor]:
    """
    Collate function –¥–ª—è DataLoader —Å BPE —Ç–æ–∫–µ–Ω–∞–º–∏.

    Args:
        batch: –°–ø–∏—Å–æ–∫ samples
        pad_token_id: ID —Ç–æ–∫–µ–Ω–∞ –¥–ª—è padding

    Returns:
        Batch –¥–∞–Ω–Ω—ã—Ö —Å padding
    """
    # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≤ batch
    max_len = max(len(sample['input_ids']) for sample in batch)

    batch_input_ids = []
    batch_labels = []
    batch_attention_mask = []

    for sample in batch:
        input_ids = sample['input_ids']
        labels = sample['labels']

        # Padding
        padding_len = max_len - len(input_ids)

        # –î–æ–±–∞–≤–ª—è–µ–º padding —Å–ø—Ä–∞–≤–∞
        padded_input = torch.cat([
            input_ids,
            torch.full((padding_len,), pad_token_id, dtype=torch.long)
        ])

        padded_labels = torch.cat([
            labels,
            torch.full((padding_len,), -100, dtype=torch.long)  # -100 –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≤ CrossEntropyLoss
        ])

        # Attention mask
        attention_mask = torch.cat([
            torch.ones(len(input_ids), dtype=torch.long),
            torch.zeros(padding_len, dtype=torch.long)
        ])

        batch_input_ids.append(padded_input)
        batch_labels.append(padded_labels)
        batch_attention_mask.append(attention_mask)

    return {
        'input_ids': torch.stack(batch_input_ids),
        'labels': torch.stack(batch_labels),
        'attention_mask': torch.stack(batch_attention_mask)
    }


def main():
    print("=" * 80)
    print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ general —Å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º")
    print("=" * 80)

    # === 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è ===
    expert_id = "general"
    tokenizer_path = project_root / "models" / "tokenizers" / "bpe_multilang.model"
    train_file = project_root / "data" / "training" / "corpus_for_bpe.txt"
    models_dir = project_root / "models" / "experts"
    expert_dir = models_dir / expert_id
    checkpoints_dir = project_root / "checkpoints" / f"{expert_id}_bpe"

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    vocab_size = 8000  # BPE vocab size
    d_model = 256
    n_layers = 4  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 2 –¥–æ 4 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    n_heads = 8    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 4 –¥–æ 8
    d_ff = 1024
    max_seq_len = 128

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_epochs = 15  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 15 —ç–ø–æ—Ö
    batch_size = 8
    learning_rate = 3e-4  # –ù–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    device = "cpu"

    print(f"\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"  –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {tokenizer_path}")
    print(f"  –î–∞—Ç–∞—Å–µ—Ç: {train_file}")
    print(f"  –ú–æ–¥–µ–ª—å: {expert_id}")
    print(f"  Vocab size: {vocab_size} (BPE)")
    print(f"  Model size: d_model={d_model}, n_layers={n_layers}, n_heads={n_heads}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Batch size: {batch_size}")
    print(f"  Learning rate: {learning_rate}")
    print(f"  Device: {device}")

    # === 2. –ó–∞–≥—Ä—É–∑–∫–∞ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ ===
    print("\n" + "=" * 80)
    print("–ó–∞–≥—Ä—É–∑–∫–∞ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞")
    print("=" * 80)

    tokenizer = BPETokenizer(str(tokenizer_path))

    print(f"\n‚úÖ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω!")
    print(f"   Vocabulary size: {len(tokenizer)}")

    # === 3. –°–æ–∑–¥–∞–Ω–∏–µ dataset –∏ dataloader ===
    print("\n" + "=" * 80)
    print("–°–æ–∑–¥–∞–Ω–∏–µ dataset")
    print("=" * 80)

    train_dataset = BPETextDataset(
        file_path=str(train_file),
        tokenizer=tokenizer,
        max_length=max_seq_len,
        stride=max_seq_len // 2
    )

    # Collate function —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º pad_token_id
    collate_with_pad = lambda batch: collate_fn(batch, pad_token_id=tokenizer.pad_token_id)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_with_pad,
        num_workers=0
    )

    print(f"\n‚úÖ Dataset —Å–æ–∑–¥–∞–Ω!")
    print(f"   Training samples: {len(train_dataset)}")
    print(f"   Training batches: {len(train_loader)}")

    # === 4. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print("\n" + "=" * 80)
    print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ExpertModel")
    print("=" * 80)

    model = ExpertModel(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff,
        max_seq_len=max_seq_len,
        dropout=0.1
    )

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞!")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB")

    # === 5. –°–æ–∑–¥–∞–Ω–∏–µ Trainer ===
    print("\n" + "=" * 80)
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer")
    print("=" * 80)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01
    )

    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=None,  # –ë–µ–∑ validation –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
        optimizer=optimizer,
        device=device,
        gradient_accumulation_steps=1,
        checkpoint_dir=str(checkpoints_dir),
        log_interval=5
    )

    print("\n‚úÖ Trainer –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!")

    # === 6. –û–±—É—á–µ–Ω–∏–µ ===
    print("\n" + "=" * 80)
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 80)

    try:
        history = trainer.train(
            num_epochs=num_epochs,
            save_every=3,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 3 —ç–ø–æ—Ö–∏
            early_stopping_patience=None
        )

        print("\n" + "=" * 80)
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("=" * 80)

        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print("\n–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"  Final Train Loss: {history['train_loss'][-1]:.4f}")
        print(f"  Final Train Perplexity: {history['train_perplexity'][-1]:.2f}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
        if len(history['train_loss']) > 1:
            initial_loss = history['train_loss'][0]
            final_loss = history['train_loss'][-1]
            improvement = ((initial_loss - final_loss) / initial_loss) * 100
            print(f"  Improvement: {improvement:.1f}% (loss —Å–Ω–∏–∑–∏–ª—Å—è —Å {initial_loss:.4f} –¥–æ {final_loss:.4f})")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º!")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1

    # === 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
    print("\n" + "=" * 80)
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    print("=" * 80)

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    expert_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = expert_dir / "model.pt"

    checkpoint = {
        'model_state_dict': model.state_dict(),
        'config': {
            'vocab_size': vocab_size,
            'd_model': d_model,
            'n_layers': n_layers,
            'n_heads': n_heads,
            'd_ff': d_ff,
            'max_seq_len': max_seq_len,
            'dropout': 0.1
        },
        'expert_id': expert_id,
        'training_history': history,
        'tokenizer_type': 'bpe',
        'tokenizer_path': str(tokenizer_path)
    }

    torch.save(checkpoint, model_path)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (—Å—Å—ã–ª–∫–∞ –Ω–∞ BPE –º–æ–¥–µ–ª—å)
    import json

    tokenizer_config_path = expert_dir / "tokenizer_config.json"

    tokenizer_config = {
        'tokenizer_type': 'bpe',
        'model_path': str(tokenizer_path.relative_to(project_root)),
        'vocab_size': vocab_size,
        'pad_token_id': tokenizer.pad_token_id,
        'unk_token_id': tokenizer.unk_token_id,
        'bos_token_id': tokenizer.bos_token_id,
        'eos_token_id': tokenizer.eos_token_id
    }

    with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_config, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {tokenizer_config_path}")

    # –û–±–Ω–æ–≤–ª—è–µ–º metadata.json
    metadata_path = expert_dir / "metadata.json"

    metadata = {
        "expert_id": expert_id,
        "name": "General Expert (BPE)",
        "version": "0.3.0-bpe",
        "type": "trained_model",
        "tokenizer": {
            "type": "bpe",
            "vocab_size": vocab_size,
            "supports_typos": True,
            "supports_multilingual": True
        },
        "architecture": {
            "vocab_size": vocab_size,
            "d_model": d_model,
            "n_layers": n_layers,
            "n_heads": n_heads,
            "d_ff": d_ff,
            "max_seq_len": max_seq_len
        },
        "parameters": {
            "total": total_params,
            "trainable": trainable_params
        },
        "metrics": {
            "final_train_loss": history['train_loss'][-1],
            "final_train_perplexity": history['train_perplexity'][-1],
            "epochs_trained": num_epochs
        },
        "training": {
            "dataset": str(train_file.relative_to(project_root)),
            "samples": len(train_dataset),
            "batch_size": batch_size,
            "learning_rate": learning_rate
        },
        "description": f"General Expert trained with BPE tokenizer on {len(train_dataset)} samples. Supports typos and multilingual text.",
        "created_by": "scripts/train_bpe.py"
    }

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Metadata –æ–±–Ω–æ–≤–ª—ë–Ω: {metadata_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
    history_path = expert_dir / "training_history.json"
    trainer.save_history(history_path)

    print("\n" + "=" * 80)
    print("üéâ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º.")
    print("=" * 80)
    print(f"\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"  ‚úÖ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä (8000 —Ç–æ–∫–µ–Ω–æ–≤)")
    print(f"  ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–ø–µ—á–∞—Ç–æ–∫")
    print(f"  ‚úÖ –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å (—Ä—É—Å—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)")
    print(f"  ‚úÖ Byte-level fallback –¥–ª—è –ª—é–±—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"\n–ú–æ–∂–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ UI: http://localhost:7860")
    print(f"–í—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ 'general' –≤ Manual —Ä–µ–∂–∏–º–µ.")

    # === 8. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
    print("\n" + "=" * 80)
    print("–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")
    print("=" * 80)

    model.eval()

    test_prompts = [
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        "Hello, how are you?",
        "–ø—Ä–≤–∏–µ—Ç –∫–∫ –¥–µ–ª–∞",  # –° –æ–ø–µ—á–∞—Ç–∫–∞–º–∏
    ]

    for prompt in test_prompts:
        print(f"\nPrompt: '{prompt}'")

        # –ö–æ–¥–∏—Ä—É–µ–º prompt
        input_ids = tokenizer.encode(prompt, add_special_tokens=True)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 10 —Ç–æ–∫–µ–Ω–æ–≤
        with torch.no_grad():
            for _ in range(10):
                logits = model(input_tensor)  # [1, seq_len, vocab_size]
                next_token_logits = logits[0, -1, :]  # [vocab_size]
                next_token = torch.argmax(next_token_logits).item()

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                input_tensor = torch.cat([
                    input_tensor,
                    torch.tensor([[next_token]], dtype=torch.long).to(device)
                ], dim=1)

                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ EOS
                if next_token == tokenizer.eos_token_id:
                    break

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        generated_ids = input_tensor[0].tolist()
        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        print(f"Generated: '{generated_text}'")

    print("\n" + "=" * 80)
    print("‚úÖ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    print("=" * 80)

    return 0


if __name__ == "__main__":
    sys.exit(main())
