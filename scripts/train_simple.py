#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏ general.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –î–∞—Ç–∞—Å–µ—Ç: data/training/simple_dialogues.jsonl
    - –ú–æ–¥–µ–ª—å: models/experts/general (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—É—é –≤–µ—Ä—Å–∏—é)
    - Epochs: 10 (–Ω–µ–±–æ–ª—å—à–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–µ—Å—Ç–∞)
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src/python –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src" / "python"))

import torch
from models.expert import ExpertModel
from training.dataset import SimpleTokenizer, create_dataloaders
from training.trainer import Trainer


def main():
    print("=" * 80)
    print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ general")
    print("=" * 80)

    # === 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è ===
    expert_id = "general"
    train_file = project_root / "data" / "training" / "simple_dialogues.jsonl"
    models_dir = project_root / "models" / "experts"
    expert_dir = models_dir / expert_id
    checkpoints_dir = project_root / "checkpoints" / expert_id

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 5000  # –£–≤–µ–ª–∏—á–∏–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    d_model = 256
    n_layers = 2
    n_heads = 4
    d_ff = 1024
    max_seq_len = 128  # –£–º–µ–Ω—å—à–∏–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

    num_epochs = 10
    batch_size = 8
    learning_rate = 5e-4
    device = "cpu"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU

    print(f"\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"  –î–∞—Ç–∞—Å–µ—Ç: {train_file}")
    print(f"  –ú–æ–¥–µ–ª—å: {expert_id}")
    print(f"  Vocab size: {vocab_size}")
    print(f"  Model size: d_model={d_model}, n_layers={n_layers}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Batch size: {batch_size}")
    print(f"  Learning rate: {learning_rate}")
    print(f"  Device: {device}")

    # === 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –∏ dataloader ===
    print("\n" + "=" * 80)
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞")
    print("=" * 80)

    train_loader, val_loader, tokenizer = create_dataloaders(
        train_file=str(train_file),
        val_file=None,  # –ü–æ–∫–∞ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        tokenizer=None,  # –°–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        batch_size=batch_size,
        max_length=max_seq_len,
        num_workers=0
    )

    actual_vocab_size = len(tokenizer)
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω!")
    print(f"   –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {actual_vocab_size}")
    print(f"   Training batches: {len(train_loader)}")

    # === 3. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print("\n" + "=" * 80)
    print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ExpertModel")
    print("=" * 80)

    model = ExpertModel(
        vocab_size=actual_vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff,
        max_seq_len=max_seq_len,
        dropout=0.1
    )

    total_params = sum(p.numel() for p in model.parameters())
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞!")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,} ({total_params * 4 / 1024 / 1024:.2f} MB)")

    # === 4. –°–æ–∑–¥–∞–Ω–∏–µ Trainer ===
    print("\n" + "=" * 80)
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer")
    print("=" * 80)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01
    )

    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        optimizer=optimizer,
        device=device,
        gradient_accumulation_steps=1,
        checkpoint_dir=str(checkpoints_dir),
        log_interval=5
    )

    print("\n‚úÖ Trainer –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!")

    # === 5. –û–±—É—á–µ–Ω–∏–µ ===
    print("\n" + "=" * 80)
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 80)

    try:
        history = trainer.train(
            num_epochs=num_epochs,
            save_every=2,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 2 —ç–ø–æ—Ö–∏
            early_stopping_patience=None  # –ë–µ–∑ early stopping
        )

        print("\n" + "=" * 80)
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("=" * 80)

        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print("\n–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"  Final Train Loss: {history['train_loss'][-1]:.4f}")
        print(f"  Final Train Perplexity: {history['train_perplexity'][-1]:.2f}")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º!")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1

    # === 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
    print("\n" + "=" * 80)
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    print("=" * 80)

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    expert_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = expert_dir / "model.pt"

    checkpoint = {
        'model_state_dict': model.state_dict(),
        'config': {
            'vocab_size': actual_vocab_size,
            'd_model': d_model,
            'n_layers': n_layers,
            'n_heads': n_heads,
            'd_ff': d_ff,
            'max_seq_len': max_seq_len,
            'dropout': 0.1
        },
        'expert_id': expert_id,
        'training_history': history
    }

    torch.save(checkpoint, model_path)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
    tokenizer_path = expert_dir / "tokenizer.json"
    import json

    tokenizer_data = {
        'vocab_size': actual_vocab_size,
        'word2idx': tokenizer.word2idx,
        'idx2word': tokenizer.idx2word,
        'pad_token_id': tokenizer.pad_token_id,
        'unk_token_id': tokenizer.unk_token_id,
        'bos_token_id': tokenizer.bos_token_id,
        'eos_token_id': tokenizer.eos_token_id
    }

    with open(tokenizer_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {tokenizer_path}")

    # –û–±–Ω–æ–≤–ª—è–µ–º metadata.json
    metadata_path = expert_dir / "metadata.json"

    metadata = {
        "expert_id": expert_id,
        "name": "General Expert",
        "version": "0.2.0-trained",
        "type": "trained_model",
        "architecture": {
            "vocab_size": actual_vocab_size,
            "d_model": d_model,
            "n_layers": n_layers,
            "n_heads": n_heads,
            "d_ff": d_ff,
            "max_seq_len": max_seq_len
        },
        "parameters": {
            "total": total_params,
            "trainable": total_params
        },
        "metrics": {
            "final_train_loss": history['train_loss'][-1],
            "final_train_perplexity": history['train_perplexity'][-1],
            "epochs_trained": num_epochs
        },
        "description": f"General Expert trained on {len(train_loader) * batch_size * num_epochs} examples",
        "created_by": "scripts/train_simple.py"
    }

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Metadata –æ–±–Ω–æ–≤–ª—ë–Ω: {metadata_path}")

    print("\n" + "=" * 80)
    print("üéâ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    print("=" * 80)
    print(f"\n–ú–æ–∂–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ UI: http://localhost:7860")
    print(f"–í—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ 'general' –≤ Manual —Ä–µ–∂–∏–º–µ.")

    return 0


if __name__ == "__main__":
    sys.exit(main())
