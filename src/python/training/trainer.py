"""
Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - Training loop —Å gradient accumulation
    - Validation loop —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    - Checkpoint saving/loading
    - Learning rate scheduling
    - Early stopping
    - Logging
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Optional, Dict, List, Callable
from pathlib import Path
import time
import math
import json


class Trainer:
    """
    Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ExpertModel.

    –í–∫–ª—é—á–∞–µ—Ç:
        - Training loop —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º backward
        - Validation —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        - Checkpoint management
        - Logging –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    """

    def __init__(
        self,
        model: nn.Module,
        train_dataloader: DataLoader,
        val_dataloader: Optional[DataLoader] = None,
        optimizer: Optional[optim.Optimizer] = None,
        criterion: Optional[nn.Module] = None,
        device: str = "cpu",
        gradient_accumulation_steps: int = 1,
        max_grad_norm: float = 1.0,
        checkpoint_dir: str = "checkpoints",
        log_interval: int = 10
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer.

        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            train_dataloader: DataLoader –¥–ª—è training
            val_dataloader: DataLoader –¥–ª—è validation (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞—ë—Ç—Å—è AdamW)
            criterion: Loss function (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CrossEntropyLoss)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cpu' –∏–ª–∏ 'cuda')
            gradient_accumulation_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            max_grad_norm: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è clipping
            checkpoint_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoints
            log_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ —à–∞–≥–∞—Ö)
        """
        self.model = model.to(device)
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = device
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm
        self.checkpoint_dir = Path(checkpoint_dir)
        self.log_interval = log_interval

        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è checkpoints
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        if optimizer is None:
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=5e-4,
                betas=(0.9, 0.999),
                weight_decay=0.01
            )
        else:
            self.optimizer = optimizer

        # Loss function
        if criterion is None:
            # CrossEntropyLoss –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç label=-100 (padding)
            self.criterion = nn.CrossEntropyLoss(ignore_index=-100)
        else:
            self.criterion = criterion

        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history: Dict[str, List[float]] = {
            'train_loss': [],
            'train_perplexity': [],
            'val_loss': [],
            'val_perplexity': [],
            'learning_rate': []
        }

        # –°—á—ë—Ç—á–∏–∫–∏
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')

    def train_epoch(self) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {'loss': ..., 'perplexity': ...}
        """
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        start_time = time.time()

        for batch_idx, batch in enumerate(self.train_dataloader):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
            input_ids = batch['input_ids'].to(self.device)          # [batch_size, seq_len]
            labels = batch['labels'].to(self.device)                # [batch_size, seq_len]
            attention_mask = batch.get('attention_mask', None)

            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)

            # Forward pass
            # –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç logits: [batch_size, seq_len, vocab_size]
            logits = self.model(input_ids)

            # –í—ã—á–∏—Å–ª—è–µ–º loss
            # Reshape –¥–ª—è CrossEntropyLoss: [batch_size * seq_len, vocab_size] –∏ [batch_size * seq_len]
            loss = self.criterion(
                logits.reshape(-1, logits.size(-1)),  # [batch*seq, vocab_size]
                labels.reshape(-1)                     # [batch*seq]
            )

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss –¥–ª—è gradient accumulation
            loss = loss / self.gradient_accumulation_steps

            # Backward pass
            loss.backward()

            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )

                # Optimizer step
                self.optimizer.step()
                self.optimizer.zero_grad()

                self.global_step += 1

            # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º loss (—É–º–Ω–æ–∂–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ accumulation_steps)
            total_loss += loss.item() * self.gradient_accumulation_steps
            num_batches += 1

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if (batch_idx + 1) % self.log_interval == 0:
                avg_loss = total_loss / num_batches
                perplexity = math.exp(min(avg_loss, 20))  # Clamp –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

                elapsed = time.time() - start_time
                batches_per_sec = (batch_idx + 1) / elapsed

                print(
                    f"  Batch {batch_idx + 1}/{len(self.train_dataloader)} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"PPL: {perplexity:.2f} | "
                    f"Speed: {batches_per_sec:.2f} batches/s"
                )

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
        avg_loss = total_loss / num_batches
        avg_perplexity = math.exp(min(avg_loss, 20))

        return {
            'loss': avg_loss,
            'perplexity': avg_perplexity
        }

    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ validation set.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {'loss': ..., 'perplexity': ...}
        """
        if self.val_dataloader is None:
            return {'loss': 0.0, 'perplexity': 0.0}

        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        for batch in self.val_dataloader:
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            attention_mask = batch.get('attention_mask', None)

            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)

            # Forward pass
            logits = self.model(input_ids)

            # Loss
            loss = self.criterion(
                logits.reshape(-1, logits.size(-1)),
                labels.reshape(-1)
            )

            total_loss += loss.item()
            num_batches += 1

        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / num_batches
        avg_perplexity = math.exp(min(avg_loss, 20))

        return {
            'loss': avg_loss,
            'perplexity': avg_perplexity
        }

    def train(
        self,
        num_epochs: int,
        save_every: int = 1,
        early_stopping_patience: Optional[int] = None
    ) -> Dict[str, List[float]]:
        """
        –ü–æ–ª–Ω—ã–π training loop –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö.

        Args:
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            save_every: –°–æ—Ö—Ä–∞–Ω—è—Ç—å checkpoint –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
            early_stopping_patience: –û—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ val_loss –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è N —ç–ø–æ—Ö

        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        print("=" * 80)
        print(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {num_epochs} —ç–ø–æ—Ö")
        print(f"Device: {self.device}")
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"Training batches: {len(self.train_dataloader)}")
        if self.val_dataloader:
            print(f"Validation batches: {len(self.val_dataloader)}")
        print("=" * 80)

        patience_counter = 0

        for epoch in range(num_epochs):
            self.current_epoch = epoch + 1

            print(f"\nEpoch {self.current_epoch}/{num_epochs}")
            print("-" * 80)

            # Training
            train_metrics = self.train_epoch()

            print(f"\n  Train Loss: {train_metrics['loss']:.4f}")
            print(f"  Train Perplexity: {train_metrics['perplexity']:.2f}")

            # Validation
            if self.val_dataloader is not None:
                val_metrics = self.validate()

                print(f"  Val Loss: {val_metrics['loss']:.4f}")
                print(f"  Val Perplexity: {val_metrics['perplexity']:.2f}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.history['val_loss'].append(val_metrics['loss'])
                self.history['val_perplexity'].append(val_metrics['perplexity'])

                # Early stopping
                if early_stopping_patience is not None:
                    if val_metrics['loss'] < self.best_val_loss:
                        self.best_val_loss = val_metrics['loss']
                        patience_counter = 0

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                        self.save_checkpoint(
                            self.checkpoint_dir / "best_model.pt",
                            is_best=True
                        )
                        print("  ‚úÖ New best model saved!")
                    else:
                        patience_counter += 1
                        print(f"  Early stopping: {patience_counter}/{early_stopping_patience}")

                        if patience_counter >= early_stopping_patience:
                            print("\n‚ö†Ô∏è  Early stopping triggered!")
                            break

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['train_perplexity'].append(train_metrics['perplexity'])
            self.history['learning_rate'].append(
                self.optimizer.param_groups[0]['lr']
            )

            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if (self.current_epoch % save_every) == 0:
                checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{self.current_epoch}.pt"
                self.save_checkpoint(checkpoint_path)
                print(f"\n  üíæ Checkpoint saved: {checkpoint_path}")

        print("\n" + "=" * 80)
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("=" * 80)

        return self.history

    def save_checkpoint(
        self,
        path: Path,
        is_best: bool = False
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint.

        Args:
            path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            is_best: –§–ª–∞–≥ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        """
        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'history': self.history,
            'best_val_loss': self.best_val_loss,
            'is_best': is_best
        }

        torch.save(checkpoint, path)

    def load_checkpoint(self, path: Path) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint.

        Args:
            path: –ü—É—Ç—å –∫ checkpoint
        """
        checkpoint = torch.load(path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.history = checkpoint.get('history', self.history)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))

        print(f"‚úÖ Checkpoint –∑–∞–≥—Ä—É–∂–µ–Ω: epoch {self.current_epoch}, step {self.global_step}")

    def save_history(self, path: Path) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ JSON.

        Args:
            path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)

        print(f"üìä –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")


# ============================================================================
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
# ============================================================================

if __name__ == "__main__":
    import sys
    sys.path.append('src/python')

    from models.expert import ExpertModel
    from training.dataset import create_dataloaders

    print("=" * 80)
    print("–¢–µ—Å—Ç Trainer")
    print("=" * 80)

    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    test_file = "/tmp/train_test.txt"
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write("""
–≠—Ç–æ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
–ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞.

–í—Ç–æ—Ä–æ–π –ø—Ä–∏–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
–ß–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, —Ç–µ–º –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è.

–¢—Ä–µ—Ç–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ dataset.
–û–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞.
""")

    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders...")
    train_loader, val_loader, tokenizer = create_dataloaders(
        train_file=test_file,
        batch_size=2,
        max_length=32
    )
    print(f"   Vocab size: {len(tokenizer)}")
    print(f"   Train batches: {len(train_loader)}")

    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = ExpertModel(
        vocab_size=len(tokenizer),
        d_model=128,
        n_layers=2,
        n_heads=4,
        d_ff=512,
        max_seq_len=32,
        dropout=0.1
    )
    print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    print("\n3. –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=None,  # –ù–µ—Ç validation –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
        device='cpu',
        gradient_accumulation_steps=2,
        checkpoint_dir='test_checkpoints',
        log_interval=1
    )

    print("\n4. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 2 —ç–ø–æ—Ö–∞—Ö...")
    history = trainer.train(
        num_epochs=2,
        save_every=1
    )

    print("\n5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏...")
    print(f"   Train losses: {history['train_loss']}")
    print(f"   Train perplexities: {history['train_perplexity']}")

    print("\n6. –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ checkpoint...")
    checkpoint_path = Path("test_checkpoints/test_checkpoint.pt")
    trainer.save_checkpoint(checkpoint_path)

    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π trainer –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º checkpoint
    new_trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        device='cpu'
    )
    new_trainer.load_checkpoint(checkpoint_path)

    print(f"   Loaded epoch: {new_trainer.current_epoch}")
    print(f"   Loaded global_step: {new_trainer.global_step}")

    print("\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏...")
    trainer.save_history(Path("test_checkpoints/history.json"))

    print("\n" + "=" * 80)
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
    print("=" * 80)

    # Cleanup
    import shutil
    shutil.rmtree('test_checkpoints', ignore_errors=True)
