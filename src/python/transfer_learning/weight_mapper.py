"""
Weight Mapper –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ mapping –≤–µ—Å–æ–≤ GGUF ‚Üí ExpertModel.

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π mapping –∏–º—ë–Ω —Ç–µ–Ω–∑–æ—Ä–æ–≤
    - Layer-by-layer transfer –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    - Resize embeddings –ø—Ä–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ vocab_size
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (Llama, Phi, Mistral)
"""

from typing import Dict, List, Optional, Tuple
import torch
import torch.nn as nn
from pathlib import Path

from .gguf_parser import GGUFParser
from .memory_manager import MemoryManager


class WeightMapper:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π mapping –≤–µ—Å–æ–≤ –∏–∑ GGUF –º–æ–¥–µ–ª–∏ –≤ ExpertModel.

    –°–æ–∑–¥–∞—ë—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–º–µ–Ω–∞–º–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤ GGUF –∏ PyTorch –º–æ–¥–µ–ª–∏,
    –∑–∞—Ç–µ–º –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –≤–µ—Å–∞ layer-by-layer –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.

    –ü—Ä–∏–º–µ—Ä:
        >>> parser = GGUFParser("phi-3-mini-q8.gguf")
        >>> model = ExpertModel(vocab_size=8000, d_model=512, n_layers=6)
        >>>
        >>> mapper = WeightMapper(parser, model)
        >>> mapping = mapper.create_mapping()
        >>> print(f"–ù–∞–π–¥–µ–Ω–æ {len(mapping)} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π")
        >>>
        >>> report = mapper.transfer_weights(layers_to_transfer=[0, 1, 2, 3])
        >>> print(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ: {len(report['transferred'])}")
    """

    def __init__(
        self,
        gguf_parser: GGUFParser,
        expert_model: nn.Module,
        memory_manager: Optional[MemoryManager] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Weight Mapper.

        Args:
            gguf_parser: GGUF parser –¥–ª—è —á—Ç–µ–Ω–∏—è —Ç–µ–Ω–∑–æ—Ä–æ–≤
            expert_model: –¶–µ–ª–µ–≤–∞—è ExpertModel
            memory_manager: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π memory manager –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è RAM
        """
        self.gguf_parser = gguf_parser
        self.expert_model = expert_model
        self.memory_manager = memory_manager or MemoryManager()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º metadata
        self.gguf_metadata = gguf_parser.get_metadata()
        self.gguf_tensors = gguf_parser.list_tensors()

        print(f"‚úÖ WeightMapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   GGUF —Ç–µ–Ω–∑–æ—Ä–æ–≤: {len(self.gguf_tensors)}")
        print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GGUF: {self.gguf_metadata.get('architecture', 'unknown')}")

    def create_mapping(self) -> Dict[str, str]:
        """
        –°–æ–∑–¥–∞—ë—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π mapping –∏–º—ë–Ω —Ç–µ–Ω–∑–æ—Ä–æ–≤ GGUF ‚Üí ExpertModel.

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {gguf_tensor_name: expert_model_param_name}

        Example:
            >>> mapping = mapper.create_mapping()
            >>> for gguf_name, model_name in mapping.items():
            >>>     print(f"{gguf_name} ‚Üí {model_name}")
        """
        mapping = {}

        # –ü–æ–ª—É—á–∞–µ–º state dict ExpertModel –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∏–º—ë–Ω
        model_state_dict = self.expert_model.state_dict()
        model_params = set(model_state_dict.keys())

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏–∑ GGUF
        arch = self.gguf_metadata.get('architecture', 'llama')

        # 1. Mapping –¥–ª—è token embeddings
        emb_mapping = self._map_embeddings(arch)
        mapping.update(emb_mapping)

        # 2. Mapping –¥–ª—è transformer –±–ª–æ–∫–æ–≤
        layers_mapping = self._map_transformer_layers(arch)
        mapping.update(layers_mapping)

        # 3. Mapping –¥–ª—è output projection (lm_head)
        output_mapping = self._map_output_head(arch)
        mapping.update(output_mapping)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤ –º–æ–¥–µ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        valid_mapping = {}
        for gguf_name, model_name in mapping.items():
            if model_name in model_params:
                valid_mapping[gguf_name] = model_name
            else:
                print(f"‚ö†Ô∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä '{model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω mapping: {len(valid_mapping)} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π")
        return valid_mapping

    def _map_embeddings(self, arch: str) -> Dict[str, str]:
        """
        Mapping –¥–ª—è token embeddings.

        Args:
            arch: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GGUF (llama, phi, mistral, etc.)

        Returns:
            –°–ª–æ–≤–∞—Ä—å mapping –¥–ª—è embeddings
        """
        mapping = {}

        # Llama/Mistral style: "token_embd.weight"
        if "token_embd.weight" in self.gguf_tensors:
            mapping["token_embd.weight"] = "token_embedding.weight"

        # Phi style: "model.embed_tokens.weight"
        elif "model.embed_tokens.weight" in self.gguf_tensors:
            mapping["model.embed_tokens.weight"] = "token_embedding.weight"

        # GPT style: "wte.weight"
        elif "wte.weight" in self.gguf_tensors:
            mapping["wte.weight"] = "token_embedding.weight"

        return mapping

    def _map_transformer_layers(self, arch: str) -> Dict[str, str]:
        """
        Mapping –¥–ª—è transformer —Å–ª–æ—ë–≤.

        Args:
            arch: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GGUF

        Returns:
            –°–ª–æ–≤–∞—Ä—å mapping –¥–ª—è –≤—Å–µ—Ö transformer —Å–ª–æ—ë–≤
        """
        mapping = {}

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤
        n_layers_gguf = self.gguf_metadata.get('n_layers', 32)
        n_layers_model = getattr(self.expert_model, 'n_layers', 6)

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤
        n_layers_to_map = min(n_layers_gguf, n_layers_model)

        for layer_idx in range(n_layers_to_map):
            # Llama/Mistral style: "blk.{i}.*"
            layer_mapping = self._map_single_layer_llama_style(layer_idx)
            if layer_mapping:
                mapping.update(layer_mapping)
                continue

            # Phi style: "model.layers.{i}.*"
            layer_mapping = self._map_single_layer_phi_style(layer_idx)
            if layer_mapping:
                mapping.update(layer_mapping)
                continue

        return mapping

    def _map_single_layer_llama_style(self, layer_idx: int) -> Dict[str, str]:
        """
        Mapping –¥–ª—è –æ–¥–Ω–æ–≥–æ transformer —Å–ª–æ—è (Llama/Mistral style).

        Args:
            layer_idx: –ò–Ω–¥–µ–∫—Å —Å–ª–æ—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å mapping –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª–æ—è
        """
        mapping = {}
        prefix_gguf = f"blk.{layer_idx}"
        prefix_model = f"transformer_blocks.{layer_idx}"

        # Attention weights
        # Q, K, V, O projections
        attn_mappings = {
            f"{prefix_gguf}.attn_q.weight": f"{prefix_model}.attention.W_q.weight",
            f"{prefix_gguf}.attn_k.weight": f"{prefix_model}.attention.W_k.weight",
            f"{prefix_gguf}.attn_v.weight": f"{prefix_model}.attention.W_v.weight",
            f"{prefix_gguf}.attn_output.weight": f"{prefix_model}.attention.W_o.weight",

            # Attention biases (–µ—Å–ª–∏ –µ—Å—Ç—å)
            f"{prefix_gguf}.attn_q.bias": f"{prefix_model}.attention.W_q.bias",
            f"{prefix_gguf}.attn_k.bias": f"{prefix_model}.attention.W_k.bias",
            f"{prefix_gguf}.attn_v.bias": f"{prefix_model}.attention.W_v.bias",
            f"{prefix_gguf}.attn_output.bias": f"{prefix_model}.attention.W_o.bias",
        }

        # FFN weights
        ffn_mappings = {
            f"{prefix_gguf}.ffn_up.weight": f"{prefix_model}.ffn.linear1.weight",
            f"{prefix_gguf}.ffn_down.weight": f"{prefix_model}.ffn.linear2.weight",

            # FFN biases (–µ—Å–ª–∏ –µ—Å—Ç—å)
            f"{prefix_gguf}.ffn_up.bias": f"{prefix_model}.ffn.linear1.bias",
            f"{prefix_gguf}.ffn_down.bias": f"{prefix_model}.ffn.linear2.bias",
        }

        # LayerNorm weights
        norm_mappings = {
            f"{prefix_gguf}.attn_norm.weight": f"{prefix_model}.norm1.weight",
            f"{prefix_gguf}.attn_norm.bias": f"{prefix_model}.norm1.bias",
            f"{prefix_gguf}.ffn_norm.weight": f"{prefix_model}.norm2.weight",
            f"{prefix_gguf}.ffn_norm.bias": f"{prefix_model}.norm2.bias",
        }

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ mappings
        all_mappings = {**attn_mappings, **ffn_mappings, **norm_mappings}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã
        for gguf_name, model_name in all_mappings.items():
            if gguf_name in self.gguf_tensors:
                mapping[gguf_name] = model_name

        return mapping

    def _map_single_layer_phi_style(self, layer_idx: int) -> Dict[str, str]:
        """
        Mapping –¥–ª—è –æ–¥–Ω–æ–≥–æ transformer —Å–ª–æ—è (Phi style).

        Args:
            layer_idx: –ò–Ω–¥–µ–∫—Å —Å–ª–æ—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å mapping –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª–æ—è
        """
        mapping = {}
        prefix_gguf = f"model.layers.{layer_idx}"
        prefix_model = f"transformer_blocks.{layer_idx}"

        # Phi –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∏–º–µ–Ω–∞
        # TODO: –î–æ–±–∞–≤–∏—Ç—å support –¥–ª—è Phi naming convention

        return mapping

    def _map_output_head(self, arch: str) -> Dict[str, str]:
        """
        Mapping –¥–ª—è output projection (lm_head).

        Args:
            arch: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GGUF

        Returns:
            –°–ª–æ–≤–∞—Ä—å mapping –¥–ª—è lm_head
        """
        mapping = {}

        # Llama/Mistral style: "output.weight"
        if "output.weight" in self.gguf_tensors:
            mapping["output.weight"] = "lm_head.weight"

        # Phi style: "lm_head.weight" (—É–∂–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
        elif "lm_head.weight" in self.gguf_tensors:
            mapping["lm_head.weight"] = "lm_head.weight"

        return mapping

    def transfer_weights(
        self,
        layers_to_transfer: Optional[List[int]] = None,
        freeze_layers: bool = False,
        resize_embeddings: bool = True
    ) -> Dict[str, List[str]]:
        """
        –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –≤–µ—Å–∞ –∏–∑ GGUF –≤ ExpertModel layer-by-layer.

        Args:
            layers_to_transfer: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ—ë–≤ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ (None = –≤—Å–µ)
            freeze_layers: –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–µ –≤–µ—Å–∞ (–Ω–µ –æ–±—É—á–∞—Ç—å)
            resize_embeddings: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ resize embeddings –µ—Å–ª–∏ vocab mismatch

        Returns:
            –û—Ç—á—ë—Ç –æ –ø–µ—Ä–µ–Ω–æ—Å–µ:
                - 'transferred': —Å–ø–∏—Å–æ–∫ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                - 'skipped': —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                - 'resized': —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å resize
                - 'frozen': —Å–ø–∏—Å–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Example:
            >>> report = mapper.transfer_weights(
            ...     layers_to_transfer=[0, 1, 2, 3],
            ...     freeze_layers=True
            ... )
            >>> print(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ: {len(report['transferred'])}")
            >>> print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(report['skipped'])}")
        """
        # –°–æ–∑–¥–∞—ë–º mapping
        mapping = self.create_mapping()

        report = {
            'transferred': [],
            'skipped': [],
            'resized': [],
            'frozen': []
        }

        # –ü–æ–ª—É—á–∞–µ–º state dict –º–æ–¥–µ–ª–∏
        model_state_dict = self.expert_model.state_dict()

        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å –≤–µ—Å–æ–≤...")
        print(f"   –í—Å–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {len(mapping)}")

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤–µ—Å–∞
        for gguf_name, model_name in mapping.items():
            try:
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–ª–æ—è–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
                if layers_to_transfer is not None:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –Ω—É–∂–Ω–æ–º—É —Å–ª–æ—é
                    if not self._should_transfer_param(model_name, layers_to_transfer):
                        report['skipped'].append(model_name)
                        continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                tensor_info = self.gguf_parser.get_tensor_info(gguf_name)
                tensor_size_bytes = tensor_info['n_elements'] * 4  # FP32

                if not self.memory_manager.can_load_tensor_size(tensor_size_bytes):
                    print(f"‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è {gguf_name}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    report['skipped'].append(model_name)
                    continue

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –∏–∑ GGUF
                gguf_tensor = self.gguf_parser.load_tensor(gguf_name, dequantize=True)

                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é —Ñ–æ—Ä–º—É –∏–∑ –º–æ–¥–µ–ª–∏
                target_param = model_state_dict[model_name]
                target_shape = target_param.shape

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
                if gguf_tensor.shape != target_shape:
                    # –ü—ã—Ç–∞–µ–º—Å—è resize –µ—Å–ª–∏ —ç—Ç–æ embeddings
                    if 'embedding' in model_name or 'lm_head' in model_name:
                        if resize_embeddings:
                            gguf_tensor = self._resize_embedding_tensor(
                                gguf_tensor,
                                target_shape,
                                model_name
                            )
                            report['resized'].append(model_name)
                        else:
                            print(f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –¥–ª—è {model_name}")
                            print(f"     GGUF: {gguf_tensor.shape}, Model: {target_shape}")
                            report['skipped'].append(model_name)
                            continue
                    else:
                        print(f"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –¥–ª—è {model_name}")
                        print(f"     GGUF: {gguf_tensor.shape}, Model: {target_shape}")
                        report['skipped'].append(model_name)
                        continue

                # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤ –º–æ–¥–µ–ª—å
                with torch.no_grad():
                    model_state_dict[model_name].copy_(gguf_tensor)

                report['transferred'].append(model_name)
                print(f"‚úÖ –ü–µ—Ä–µ–Ω–µ—Å—ë–Ω: {model_name}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å–µ {gguf_name}: {str(e)}")
                report['skipped'].append(model_name)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π state dict
        self.expert_model.load_state_dict(model_state_dict)

        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if freeze_layers:
            frozen = self._freeze_transferred_params(report['transferred'])
            report['frozen'] = frozen

        print(f"\n‚úÖ –ü–µ—Ä–µ–Ω–æ—Å –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print(f"   –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ: {len(report['transferred'])}")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(report['skipped'])}")
        print(f"   Resized: {len(report['resized'])}")
        print(f"   –ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ: {len(report['frozen'])}")

        return report

    def _should_transfer_param(self, param_name: str, layers_to_transfer: List[int]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ —Å–ª–æ—ë–≤.

        Args:
            param_name: –ò–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤ –º–æ–¥–µ–ª–∏
            layers_to_transfer: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ—ë–≤ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞

        Returns:
            True –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å
        """
        # Embeddings –∏ lm_head –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏–º
        if 'embedding' in param_name or 'lm_head' in param_name:
            return True

        # –î–ª—è transformer –±–ª–æ–∫–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å
        if 'transformer_blocks' in param_name:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å–ª–æ—è: transformer_blocks.{idx}.*
            parts = param_name.split('.')
            if len(parts) >= 2:
                try:
                    layer_idx = int(parts[1])
                    return layer_idx in layers_to_transfer
                except ValueError:
                    pass

        return True

    def _resize_embedding_tensor(
        self,
        source_tensor: torch.Tensor,
        target_shape: tuple,
        param_name: str
    ) -> torch.Tensor:
        """
        Resize embedding —Ç–µ–Ω–∑–æ—Ä–∞ –ø—Ä–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ vocab_size.

        Args:
            source_tensor: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä –∏–∑ GGUF
            target_shape: –¶–µ–ª–µ–≤–∞—è —Ñ–æ—Ä–º–∞
            param_name: –ò–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)

        Returns:
            Resized —Ç–µ–Ω–∑–æ—Ä
        """
        print(f"üîß Resize {param_name}: {source_tensor.shape} ‚Üí {target_shape}")

        # –î–ª—è embeddings: [vocab_size, d_model]
        if len(source_tensor.shape) == 2 and len(target_shape) == 2:
            source_vocab, source_dim = source_tensor.shape
            target_vocab, target_dim = target_shape

            # –ï—Å–ª–∏ d_model —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–µ–∑–∞–µ–º/–¥–æ–ø–æ–ª–Ω—è–µ–º vocabulary
            if source_dim == target_dim:
                if target_vocab < source_vocab:
                    # –û–±—Ä–µ–∑–∞–µ–º
                    return source_tensor[:target_vocab, :]
                else:
                    # –î–æ–ø–æ–ª–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
                    new_tensor = torch.randn(target_vocab, target_dim) * 0.02
                    new_tensor[:source_vocab, :] = source_tensor
                    return new_tensor

        # Fallback: –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º
        new_tensor = torch.randn(*target_shape) * 0.02

        # –ö–æ–ø–∏—Ä—É–µ–º –æ–±—â—É—é —á–∞—Å—Ç—å
        slices = tuple(slice(0, min(s, t)) for s, t in zip(source_tensor.shape, target_shape))
        new_tensor[slices] = source_tensor[slices]

        return new_tensor

    def _freeze_transferred_params(self, param_names: List[str]) -> List[str]:
        """
        –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (requires_grad = False).

        Args:
            param_names: –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∑–∞–º–æ—Ä–æ–∑–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        """
        frozen = []

        for name, param in self.expert_model.named_parameters():
            if name in param_names:
                param.requires_grad = False
                frozen.append(name)

        return frozen
