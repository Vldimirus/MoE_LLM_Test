"""
Tokenizer Aligner –¥–ª—è alignment vocabulary –º–µ–∂–¥—É GGUF –∏ BPE.

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –°–æ–∑–¥–∞–Ω–∏–µ mapping –º–µ–∂–¥—É GGUF vocab –∏ BPE vocab
    - Alignment embeddings –ø—Ä–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ vocab_size
    - –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
    - Random initialization –¥–ª—è unmapped —Ç–æ–∫–µ–Ω–æ–≤
"""

from typing import Dict, List, Tuple, Optional
import torch
import numpy as np


class TokenizerAligner:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç alignment –º–µ–∂–¥—É vocabulary GGUF –º–æ–¥–µ–ª–∏ –∏ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞.

    GGUF –º–æ–¥–µ–ª–∏ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç vocab 32k-128k —Ç–æ–∫–µ–Ω–æ–≤,
    –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ BPE –º–æ–∂–µ—Ç –∏–º–µ—Ç—å 8k —Ç–æ–∫–µ–Ω–æ–≤.
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Å–æ–∑–¥–∞—ë—Ç mapping –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç embeddings.

    –ü—Ä–∏–º–µ—Ä:
        >>> parser = GGUFParser("phi-3-mini-q8.gguf")
        >>> bpe = BPETokenizer.from_file("bpe_multilang.model")
        >>>
        >>> aligner = TokenizerAligner(parser, bpe)
        >>> mapping = aligner.create_vocab_mapping()
        >>> print(f"Vocab overlap: {len(mapping)} / {len(bpe)}")
        >>>
        >>> # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º embeddings –∏–∑ GGUF
        >>> gguf_embeddings = parser.load_tensor("token_embd.weight")
        >>> aligned_emb = aligner.align_embeddings(gguf_embeddings, 8000)
    """

    def __init__(self, gguf_parser, bpe_tokenizer=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Tokenizer Aligner.

        Args:
            gguf_parser: GGUFParser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è vocabulary
            bpe_tokenizer: BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.gguf_parser = gguf_parser
        self.bpe_tokenizer = bpe_tokenizer

        # –ò–∑–≤–ª–µ–∫–∞–µ–º vocabulary –∏–∑ GGUF
        self.gguf_vocab = gguf_parser.get_vocab()
        self.gguf_vocab_size = len(self.gguf_vocab) if self.gguf_vocab else gguf_parser.get_metadata().get('vocab_size', 32000)

        # BPE vocabulary
        if bpe_tokenizer:
            self.bpe_vocab = self._get_bpe_vocab(bpe_tokenizer)
            self.bpe_vocab_size = len(self.bpe_vocab)
        else:
            self.bpe_vocab = []
            self.bpe_vocab_size = 8000  # Default

        print(f"‚úÖ TokenizerAligner –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   GGUF vocab size: {self.gguf_vocab_size}")
        print(f"   BPE vocab size: {self.bpe_vocab_size}")

    def _get_bpe_vocab(self, bpe_tokenizer) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç vocabulary –∏–∑ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞.

        Args:
            bpe_tokenizer: BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è vocab
            if hasattr(bpe_tokenizer, 'get_vocab'):
                return list(bpe_tokenizer.get_vocab().keys())
            elif hasattr(bpe_tokenizer, 'vocab'):
                return list(bpe_tokenizer.vocab)
            elif hasattr(bpe_tokenizer, 'id_to_piece'):
                # SentencePiece style
                return [bpe_tokenizer.id_to_piece(i) for i in range(len(bpe_tokenizer))]
            else:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å BPE vocabulary")
                return []
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ BPE vocab: {e}")
            return []

    def create_vocab_mapping(self) -> Dict[int, int]:
        """
        –°–æ–∑–¥–∞—ë—Ç mapping –º–µ–∂–¥—É GGUF token ID –∏ BPE token ID.

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
            - –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é
            - –î–ª—è —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–æ–∑–¥–∞—ë–º mapping
            - –î–ª—è –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –æ—Å—Ç–∞–≤–ª—è–µ–º None (random init)

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {gguf_token_id: bpe_token_id}

        Example:
            >>> mapping = aligner.create_vocab_mapping()
            >>> print(f"Mapped {len(mapping)} tokens")
            >>> print(f"Overlap: {len(mapping) / aligner.bpe_vocab_size:.1%}")
        """
        if not self.gguf_vocab or not self.bpe_vocab:
            print("‚ö†Ô∏è Vocabulary –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º mapping")
            return {}

        mapping = {}

        # –°–æ–∑–¥–∞—ë–º –æ–±—Ä–∞—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è BPE vocab –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        bpe_token_to_id = {token: idx for idx, token in enumerate(self.bpe_vocab)}

        matched_count = 0

        for gguf_id, gguf_token in enumerate(self.gguf_vocab):
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized_token = self._normalize_token(gguf_token)

            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ BPE
            if normalized_token in bpe_token_to_id:
                bpe_id = bpe_token_to_id[normalized_token]
                mapping[gguf_id] = bpe_id
                matched_count += 1

        overlap_percent = (matched_count / self.bpe_vocab_size) * 100 if self.bpe_vocab_size > 0 else 0

        print(f"‚úÖ Vocab mapping —Å–æ–∑–¥–∞–Ω:")
        print(f"   –°–æ–≤–ø–∞–ª–æ —Ç–æ–∫–µ–Ω–æ–≤: {matched_count}")
        print(f"   Overlap: {overlap_percent:.1f}%")

        return mapping

    def _normalize_token(self, token: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–æ–∫–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

        Args:
            token: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫–µ–Ω

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω
        """
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã/—Å—É—Ñ—Ñ–∏–∫—Å—ã
        token = token.strip()

        # –£–±–∏—Ä–∞–µ–º BPE markers (‚ñÅ, ƒ†, etc.)
        token = token.replace('‚ñÅ', ' ')  # SentencePiece
        token = token.replace('ƒ†', ' ')   # GPT-2 style

        return token

    def align_embeddings(
        self,
        gguf_embeddings: torch.Tensor,
        target_vocab_size: int,
        vocab_mapping: Optional[Dict[int, int]] = None
    ) -> torch.Tensor:
        """
        –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç GGUF embeddings –ø–æ–¥ —Ü–µ–ª–µ–≤–æ–π vocabulary size.

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
            1. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É [target_vocab_size, d_model]
            2. –î–ª—è mapped —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –∏–∑ GGUF
            3. –î–ª—è unmapped —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º random initialization

        Args:
            gguf_embeddings: Embeddings –∏–∑ GGUF [gguf_vocab_size, d_model]
            target_vocab_size: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä vocabulary (–æ–±—ã—á–Ω–æ BPE size)
            vocab_mapping: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π mapping (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

        Returns:
            Aligned embeddings [target_vocab_size, d_model]

        Example:
            >>> gguf_emb = parser.load_tensor("token_embd.weight")  # [32000, 4096]
            >>> aligned_emb = aligner.align_embeddings(gguf_emb, 8000)  # [8000, 4096]
            >>> print(f"Aligned embeddings: {aligned_emb.shape}")
        """
        gguf_vocab_size, d_model = gguf_embeddings.shape

        print(f"\nüîß Alignment embeddings:")
        print(f"   Source: {gguf_embeddings.shape}")
        print(f"   Target: ({target_vocab_size}, {d_model})")

        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ
        if gguf_vocab_size == target_vocab_size:
            print(f"   ‚úÖ –†–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç, alignment –Ω–µ –Ω—É–∂–µ–Ω")
            return gguf_embeddings

        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É embeddings
        aligned_embeddings = torch.randn(target_vocab_size, d_model) * 0.02

        # –°–æ–∑–¥–∞—ë–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π mapping
        if vocab_mapping is None:
            vocab_mapping = self.create_vocab_mapping()

        # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –¥–ª—è mapped —Ç–æ–∫–µ–Ω–æ–≤
        mapped_count = 0
        for gguf_id, bpe_id in vocab_mapping.items():
            if gguf_id < gguf_vocab_size and bpe_id < target_vocab_size:
                aligned_embeddings[bpe_id] = gguf_embeddings[gguf_id]
                mapped_count += 1

        unmapped_count = target_vocab_size - mapped_count

        print(f"   ‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {mapped_count} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   üîÄ Random init: {unmapped_count} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   üìä Overlap: {(mapped_count / target_vocab_size) * 100:.1f}%")

        return aligned_embeddings

    def estimate_vocab_overlap(self) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç overlap –º–µ–∂–¥—É GGUF –∏ BPE vocabulary.

        Returns:
            –ü—Ä–æ—Ü–µ–Ω—Ç overlap (0.0 - 1.0)

        Example:
            >>> overlap = aligner.estimate_vocab_overlap()
            >>> print(f"Vocab overlap: {overlap:.1%}")
        """
        if not self.gguf_vocab or not self.bpe_vocab:
            # –ï—Å–ª–∏ vocab –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–∞—ë–º –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É
            return 0.6  # ~60% overlap —Ç–∏–ø–∏—á–µ–Ω –¥–ª—è —Ä–∞–∑–Ω—ã—Ö tokenizers

        mapping = self.create_vocab_mapping()
        overlap = len(mapping) / self.bpe_vocab_size if self.bpe_vocab_size > 0 else 0.0

        return overlap

    def get_special_tokens_mapping(self) -> Dict[str, Tuple[int, int]]:
        """
        –°–æ–∑–¥–∞—ë—Ç mapping –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (PAD, UNK, BOS, EOS).

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {token_name: (gguf_id, bpe_id)}

        Example:
            >>> special = aligner.get_special_tokens_mapping()
            >>> print(f"PAD token: GGUF={special['PAD'][0]}, BPE={special['PAD'][1]}")
        """
        special_mapping = {}

        # –¢–∏–ø–∏—á–Ω—ã–µ –∏–º–µ–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        special_names = {
            'PAD': ['<pad>', '<|pad|>', '[PAD]'],
            'UNK': ['<unk>', '<|unk|>', '[UNK]'],
            'BOS': ['<s>', '<|startoftext|>', '[BOS]', '<bos>'],
            'EOS': ['</s>', '<|endoftext|>', '[EOS]', '<eos>'],
        }

        for token_type, candidates in special_names.items():
            # –ò—â–µ–º –≤ GGUF vocab
            gguf_id = self._find_token_id(candidates, self.gguf_vocab)

            # –ò—â–µ–º –≤ BPE vocab
            bpe_id = self._find_token_id(candidates, self.bpe_vocab)

            if gguf_id is not None and bpe_id is not None:
                special_mapping[token_type] = (gguf_id, bpe_id)

        return special_mapping

    def _find_token_id(self, candidates: List[str], vocab: List[str]) -> Optional[int]:
        """
        –ò—â–µ—Ç —Ç–æ–∫–µ–Ω –≤ vocabulary –ø–æ —Å–ø–∏—Å–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.

        Args:
            candidates: –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏–º—ë–Ω —Ç–æ–∫–µ–Ω–∞
            vocab: Vocabulary –¥–ª—è –ø–æ–∏—Å–∫–∞

        Returns:
            ID —Ç–æ–∫–µ–Ω–∞ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        for candidate in candidates:
            if candidate in vocab:
                return vocab.index(candidate)

        return None
