"""
Transfer Learning Pipeline –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ExpertModel –∏–∑ GGUF –º–æ–¥–µ–ª–µ–π.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç high-level API –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ transfer learning:
    - –ü–∞—Ä—Å–∏–Ω–≥ GGUF —Ñ–∞–π–ª–∞
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    - Alignment vocabulary
    - –ü–µ—Ä–µ–Ω–æ—Å –≤–µ—Å–æ–≤
    - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ExpertModel

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    >>> from python.transfer_learning import TransferLearningPipeline
    >>>
    >>> pipeline = TransferLearningPipeline(
    ...     gguf_path="models/gguf/phi-3-mini-q8.gguf",
    ...     target_model_config={
    ...         'vocab_size': 8000,
    ...         'd_model': 512,
    ...         'n_layers': 6,
    ...         'n_heads': 8,
    ...         'd_ff': 2048
    ...     },
    ...     bpe_tokenizer_path="models/tokenizers/bpe_multilang.model"
    ... )
    >>>
    >>> # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    >>> compat = pipeline.validate_compatibility()
    >>> print(f"Vocab overlap: {compat['vocab_overlap']:.1%}")
    >>>
    >>> # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    >>> model = pipeline.initialize_model_from_gguf(
    ...     layers_to_transfer=[0, 1, 2, 3, 4, 5],
    ...     freeze_transferred_layers=True
    ... )
    >>>
    >>> # –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ fine-tuning!
"""

from typing import Dict, List, Optional, Any
from pathlib import Path
import torch

from .gguf_parser import GGUFParser
from .weight_mapper import WeightMapper
from .tokenizer_aligner import TokenizerAligner
from .memory_manager import MemoryManager

# Import ExpertModel
import sys
sys.path.append(str(Path(__file__).parent.parent))
from models.expert import ExpertModel


class TransferLearningPipeline:
    """
    –ì–ª–∞–≤–Ω—ã–π API –¥–ª—è transfer learning –∏–∑ GGUF –º–æ–¥–µ–ª–µ–π.

    –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (GGUFParser, WeightMapper, TokenizerAligner, MemoryManager)
    –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ GGUF –º–æ–¥–µ–ª–∏ –≤ ExpertModel.

    Attributes:
        gguf_path: –ü—É—Ç—å –∫ GGUF —Ñ–∞–π–ª—É
        target_model_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ (vocab_size, d_model, etc.)
        bpe_tokenizer: BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        max_ram_gb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM

    Example:
        >>> pipeline = TransferLearningPipeline(
        ...     gguf_path="phi-3-mini-q8.gguf",
        ...     target_model_config={'vocab_size': 8000, 'd_model': 512, 'n_layers': 6}
        ... )
        >>>
        >>> model = pipeline.initialize_model_from_gguf()
    """

    def __init__(
        self,
        gguf_path: str,
        target_model_config: Dict[str, Any],
        bpe_tokenizer_path: Optional[str] = None,
        max_ram_gb: float = 12.0
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Transfer Learning Pipeline.

        Args:
            gguf_path: –ü—É—Ç—å –∫ GGUF —Ñ–∞–π–ª—É
            target_model_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
                –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: vocab_size, d_model, n_layers, n_heads
                –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ: d_ff (default 4*d_model), max_seq_len (default 2048), dropout (default 0.1)
            bpe_tokenizer_path: –ü—É—Ç—å –∫ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_ram_gb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM –≤ GB

        Raises:
            FileNotFoundError: –ï—Å–ª–∏ GGUF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
            ValueError: –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
        """
        self.gguf_path = Path(gguf_path)
        self.target_model_config = target_model_config
        self.bpe_tokenizer_path = bpe_tokenizer_path
        self.max_ram_gb = max_ram_gb

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._validate_config()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        print(f"\n{'='*60}")
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transfer Learning Pipeline")
        print(f"{'='*60}")

        # 1. GGUF Parser
        self.gguf_parser = GGUFParser(str(self.gguf_path))
        self.gguf_metadata = self.gguf_parser.get_metadata()

        # 2. Memory Manager
        self.memory_manager = MemoryManager(
            max_ram_gb=max_ram_gb,
            safety_margin_gb=2.0
        )

        # 3. BPE Tokenizer (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        self.bpe_tokenizer = None
        if bpe_tokenizer_path:
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                # TODO: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º
                print(f"‚ö†Ô∏è BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω (–±—É–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏)")
                self.bpe_tokenizer = None
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {e}")
                self.bpe_tokenizer = None

        # 4. Tokenizer Aligner
        self.tokenizer_aligner = TokenizerAligner(
            gguf_parser=self.gguf_parser,
            bpe_tokenizer=self.bpe_tokenizer
        )

        print(f"{'='*60}\n")

    def _validate_config(self):
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç target_model_config.

        Raises:
            ValueError: –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
        """
        required_keys = ['vocab_size', 'd_model', 'n_layers', 'n_heads']

        for key in required_keys:
            if key not in self.target_model_config:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª—é—á –≤ config: '{key}'")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è
        if self.target_model_config['vocab_size'] <= 0:
            raise ValueError("vocab_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

        if self.target_model_config['d_model'] <= 0:
            raise ValueError("d_model –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

        if self.target_model_config['n_layers'] <= 0:
            raise ValueError("n_layers –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

        if self.target_model_config['n_heads'] <= 0:
            raise ValueError("n_heads –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'd_ff' not in self.target_model_config:
            self.target_model_config['d_ff'] = 4 * self.target_model_config['d_model']

        if 'max_seq_len' not in self.target_model_config:
            self.target_model_config['max_seq_len'] = 2048

        if 'dropout' not in self.target_model_config:
            self.target_model_config['dropout'] = 0.1

    def validate_compatibility(self) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å GGUF –º–æ–¥–µ–ª–∏ —Å target config.

        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏:
            - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ (d_model, n_layers)
            - Vocab overlap –º–µ–∂–¥—É GGUF –∏ BPE
            - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –¥–ª—è transfer

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:
                - compatible: bool - –æ–±—â–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                - warnings: List[str] - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
                - vocab_overlap: float - –ø—Ä–æ—Ü–µ–Ω—Ç overlap vocabulary (0.0-1.0)
                - transferable_layers: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞
                - memory_estimate: Dict - –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏

        Example:
            >>> compat = pipeline.validate_compatibility()
            >>> if compat['compatible']:
            ...     print(f"‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ! Vocab overlap: {compat['vocab_overlap']:.1%}")
            ... else:
            ...     print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:", compat['warnings'])
        """
        print(f"\n{'='*60}")
        print("üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ GGUF –º–æ–¥–µ–ª–∏ —Å target config")
        print(f"{'='*60}\n")

        warnings = []
        compatible = True

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_model
        gguf_d_model = self.gguf_metadata.get('d_model', 0)
        target_d_model = self.target_model_config['d_model']

        print(f"d_model: GGUF={gguf_d_model}, Target={target_d_model}")

        if gguf_d_model != target_d_model:
            warnings.append(
                f"d_model –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: GGUF={gguf_d_model}, Target={target_d_model}. "
                f"–ü–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è interpolation –∏–ª–∏ truncation."
            )
            print(f"‚ö†Ô∏è d_model mismatch: –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è")
        else:
            print(f"‚úÖ d_model —Å–æ–≤–ø–∞–¥–∞–µ—Ç")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ—ë–≤
        gguf_n_layers = self.gguf_metadata.get('n_layers', 0)
        target_n_layers = self.target_model_config['n_layers']

        print(f"\nn_layers: GGUF={gguf_n_layers}, Target={target_n_layers}")

        transferable_layers = min(gguf_n_layers, target_n_layers)

        if gguf_n_layers < target_n_layers:
            warnings.append(
                f"GGUF –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –º–µ–Ω—å—à–µ —Å–ª–æ—ë–≤ ({gguf_n_layers}) —á–µ–º target ({target_n_layers}). "
                f"–ë—É–¥—É—Ç –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {transferable_layers} —Å–ª–æ—ë–≤, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - random init."
            )
            print(f"‚ö†Ô∏è GGUF —Å–ª–æ—ë–≤ –º–µ–Ω—å—à–µ: –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ —Ç–æ–ª—å–∫–æ {transferable_layers}")
        elif gguf_n_layers > target_n_layers:
            warnings.append(
                f"GGUF –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –±–æ–ª—å—à–µ —Å–ª–æ—ë–≤ ({gguf_n_layers}) —á–µ–º target ({target_n_layers}). "
                f"–ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {transferable_layers} —Å–ª–æ—ë–≤."
            )
            print(f"‚ö†Ô∏è GGUF —Å–ª–æ—ë–≤ –±–æ–ª—å—à–µ: –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {transferable_layers}")
        else:
            print(f"‚úÖ n_layers —Å–æ–≤–ø–∞–¥–∞–µ—Ç")

        # 3. Vocab overlap
        vocab_overlap = self.tokenizer_aligner.estimate_vocab_overlap()

        print(f"\nVocab overlap: {vocab_overlap:.1%}")

        if vocab_overlap < 0.3:
            warnings.append(
                f"–ù–∏–∑–∫–∏–π vocab overlap ({vocab_overlap:.1%}). "
                f"–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥—É—Ç random initialized."
            )
            print(f"‚ö†Ô∏è –ù–∏–∑–∫–∏–π vocab overlap: –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∏–∑–∫–∏–º")
        elif vocab_overlap < 0.5:
            warnings.append(
                f"–£–º–µ—Ä–µ–Ω–Ω—ã–π vocab overlap ({vocab_overlap:.1%}). "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤–∞—à–µ–º –¥–æ–º–µ–Ω–µ."
            )
            print(f"‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π vocab overlap")
        else:
            print(f"‚úÖ –•–æ—Ä–æ—à–∏–π vocab overlap")

        # 4. –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏
        memory_estimate = self.memory_manager.estimate_model_memory(
            n_layers=target_n_layers,
            d_model=target_d_model,
            vocab_size=self.target_model_config['vocab_size'],
            d_ff=self.target_model_config.get('d_ff')
        )

        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏:")
        print(f"   –ú–æ–¥–µ–ª—å (inference): {memory_estimate['inference_gb']:.2f} GB")
        print(f"   –ú–æ–¥–µ–ª—å (training):  {memory_estimate['training_gb']:.2f} GB")
        print(f"   –î–æ—Å—Ç—É–ø–Ω–æ RAM:       {memory_estimate['available_gb']:.2f} GB")

        if not memory_estimate['can_fit_inference']:
            warnings.append(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM –¥–ª—è inference: —Ç—Ä–µ–±—É–µ—Ç—Å—è {memory_estimate['inference_gb']:.2f} GB, "
                f"–¥–æ—Å—Ç—É–ø–Ω–æ {memory_estimate['available_gb']:.2f} GB."
            )
            compatible = False
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è inference!")

        if not memory_estimate['can_fit_training']:
            warnings.append(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM –¥–ª—è training: —Ç—Ä–µ–±—É–µ—Ç—Å—è {memory_estimate['training_gb']:.2f} GB, "
                f"–¥–æ—Å—Ç—É–ø–Ω–æ {memory_estimate['available_gb']:.2f} GB. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient checkpointing –∏–ª–∏ LoRA."
            )
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è training (–Ω–æ inference –≤–æ–∑–º–æ–∂–µ–Ω)")

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        gguf_arch = self.gguf_metadata.get('architecture', 'unknown')
        print(f"\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GGUF: {gguf_arch}")

        if gguf_arch not in ['llama', 'mistral', 'phi', 'qwen']:
            warnings.append(
                f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {gguf_arch}. "
                f"Transfer learning –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. "
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: llama, mistral, phi, qwen."
            )
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã")
        else:
            print(f"‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

        # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"\n{'='*60}")
        if compatible:
            print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–∞! Transfer learning –≤–æ–∑–º–æ–∂–µ–Ω.")
        else:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞! –¢—Ä–µ–±—É–µ—Ç—Å—è –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")

        if warnings:
            print(f"\n‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è ({len(warnings)}):")
            for i, warning in enumerate(warnings, 1):
                print(f"   {i}. {warning}")

        print(f"{'='*60}\n")

        return {
            'compatible': compatible,
            'warnings': warnings,
            'vocab_overlap': vocab_overlap,
            'transferable_layers': transferable_layers,
            'memory_estimate': memory_estimate,
            'gguf_metadata': {
                'architecture': gguf_arch,
                'd_model': gguf_d_model,
                'n_layers': gguf_n_layers,
                'vocab_size': self.gguf_metadata.get('vocab_size', 0)
            }
        }

    def initialize_model_from_gguf(
        self,
        layers_to_transfer: Optional[List[int]] = None,
        freeze_transferred_layers: bool = True,
        align_embeddings: bool = True
    ) -> ExpertModel:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ExpertModel —Å –≤–µ—Å–∞–º–∏ –∏–∑ GGUF –º–æ–¥–µ–ª–∏.

        –ü–æ–ª–Ω—ã–π pipeline:
            1. –°–æ–∑–¥–∞—ë–º ExpertModel —Å target_model_config
            2. –°–æ–∑–¥–∞—ë–º weight mapping –º–µ–∂–¥—É GGUF –∏ ExpertModel
            3. Align embeddings (–µ—Å–ª–∏ align_embeddings=True)
            4. –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤–µ—Å–∞ layer-by-layer (memory efficient)
            5. –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–µ —Å–ª–æ–∏ (–µ—Å–ª–∏ freeze_transferred_layers=True)

        Args:
            layers_to_transfer: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ—ë–≤ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞
                –ï—Å–ª–∏ None, –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ–∏
            freeze_transferred_layers: –ï—Å–ª–∏ True, –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–µ –≤–µ—Å–∞
            align_embeddings: –ï—Å–ª–∏ True, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç embeddings –ø–æ–¥ BPE vocab

        Returns:
            ExpertModel —Å –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏–∑ GGUF

        Raises:
            RuntimeError: –ï—Å–ª–∏ transfer weights –Ω–µ —É–¥–∞–ª—Å—è

        Example:
            >>> model = pipeline.initialize_model_from_gguf(
            ...     layers_to_transfer=[0, 1, 2, 3],
            ...     freeze_transferred_layers=True
            ... )
            >>> # –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ fine-tuning!
        """
        print(f"\n{'='*60}")
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ExpertModel –∏–∑ GGUF –º–æ–¥–µ–ª–∏")
        print(f"{'='*60}\n")

        # 1. –°–æ–∑–¥–∞—ë–º ExpertModel
        print("1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ ExpertModel —Å target –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π...")

        model = ExpertModel(
            vocab_size=self.target_model_config['vocab_size'],
            d_model=self.target_model_config['d_model'],
            n_layers=self.target_model_config['n_layers'],
            n_heads=self.target_model_config['n_heads'],
            d_ff=self.target_model_config.get('d_ff', 4 * self.target_model_config['d_model']),
            max_seq_len=self.target_model_config.get('max_seq_len', 2048),
            dropout=self.target_model_config.get('dropout', 0.1)
        )

        print(f"‚úÖ ExpertModel —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
        print(f"   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {self.target_model_config}")

        # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∏ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞
        if layers_to_transfer is None:
            gguf_n_layers = self.gguf_metadata.get('n_layers', 0)
            target_n_layers = self.target_model_config['n_layers']
            max_layers = min(gguf_n_layers, target_n_layers)
            layers_to_transfer = list(range(max_layers))

        print(f"\n2Ô∏è‚É£ –°–ª–æ–∏ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞: {layers_to_transfer}")

        # 3. –°–æ–∑–¥–∞—ë–º WeightMapper
        print(f"\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ weight mapping GGUF ‚Üí ExpertModel...")

        weight_mapper = WeightMapper(
            gguf_parser=self.gguf_parser,
            expert_model=model,
            memory_manager=self.memory_manager
        )

        mapping = weight_mapper.create_mapping()
        print(f"‚úÖ Mapping —Å–æ–∑–¥–∞–Ω: {len(mapping)} —Ç–µ–Ω–∑–æ—Ä–æ–≤")

        # 4. Align embeddings (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if align_embeddings:
            print(f"\n4Ô∏è‚É£ Alignment embeddings –¥–ª—è BPE vocabulary...")
            vocab_mapping = self.tokenizer_aligner.create_vocab_mapping()
            print(f"‚úÖ Vocab mapping —Å–æ–∑–¥–∞–Ω: {len(vocab_mapping)} —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤")

        # 5. –ü–µ—Ä–µ–Ω–æ—Å –≤–µ—Å–æ–≤
        print(f"\n5Ô∏è‚É£ –ü–µ—Ä–µ–Ω–æ—Å –≤–µ—Å–æ–≤ –∏–∑ GGUF –≤ ExpertModel...")

        transfer_report = weight_mapper.transfer_weights(
            layers_to_transfer=layers_to_transfer,
            freeze_layers=freeze_transferred_layers,
            resize_embeddings=align_embeddings
        )

        print(f"\n‚úÖ Transfer weights –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print(f"   –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ: {len(transfer_report['transferred'])} —Ç–µ–Ω–∑–æ—Ä–æ–≤")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(transfer_report['skipped'])} —Ç–µ–Ω–∑–æ—Ä–æ–≤")
        print(f"   Resized: {len(transfer_report['resized'])} —Ç–µ–Ω–∑–æ—Ä–æ–≤")

        if freeze_transferred_layers:
            print(f"   –ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ: {len(transfer_report['frozen'])} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

        # 6. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())

        print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏:")
        print(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params / 1e6:.2f}M")
        print(f"   Trainable: {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.1f}%)")
        print(f"   Frozen: {(total_params - trainable_params) / 1e6:.2f}M ({100 * (total_params - trainable_params) / total_params:.1f}%)")

        print(f"\n{'='*60}")
        print("üéâ ExpertModel —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ GGUF!")
        print("   –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ fine-tuning –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
        print(f"{'='*60}\n")

        return model

    def get_memory_stats(self) -> Dict[str, float]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–∞–º—è—Ç–∏.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞–º—è—Ç–∏ (total_gb, available_gb, used_gb, etc.)

        Example:
            >>> stats = pipeline.get_memory_stats()
            >>> print(f"–î–æ—Å—Ç—É–ø–Ω–æ RAM: {stats['available_gb']:.2f} GB")
        """
        return self.memory_manager.get_memory_stats()

    def print_memory_report(self):
        """
        –í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–∞–º—è—Ç–∏.

        Example:
            >>> pipeline.print_memory_report()
        """
        self.memory_manager.print_memory_report()

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"TransferLearningPipeline(\n"
            f"  gguf_path='{self.gguf_path.name}',\n"
            f"  target_config={self.target_model_config},\n"
            f"  max_ram_gb={self.max_ram_gb}\n"
            f")"
        )
